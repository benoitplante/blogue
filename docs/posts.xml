<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Benoit Plante</title>
<link>https://benoitplante.github.io/posts.html</link>
<atom:link href="https://benoitplante.github.io/posts.xml" rel="self" type="application/rss+xml"/>
<description>Réflexions, cas d’usage et outils à l’interface entre IA et psychologie</description>
<generator>quarto-1.7.31</generator>
<lastBuildDate>Thu, 19 Jun 2025 04:00:00 GMT</lastBuildDate>
<item>
  <title>L’intelligence artificielle transforme-t-elle la recherche en sciences sociales ?</title>
  <dc:creator>Benoit Plante + ChatGPT</dc:creator>
  <link>https://benoitplante.github.io/posts/2025-06-19-AI-sciences-sociales/</link>
  <description><![CDATA[ 





<section id="en-bref" class="level2">
<h2 class="anchored" data-anchor-id="en-bref">En bref</h2>
<p>Une étude récente explore comment les chercheurs en sciences sociales perçoivent et utilisent l’intelligence artificielle (IA) et l’apprentissage machine (ML). Basée sur une enquête (N = 284) et des entretiens (N = 15), elle révèle une adoption croissante, mais ambivalente, de ces outils. Les chercheurs utilisent davantage l’IA générative (genAI), mais lui font moins confiance que le ML, qu’ils jugent plus transparent. L’article plaide pour une intégration centrée sur l’humain et propose des recommandations pour les développeurs, chercheurs, enseignants et décideurs. Ce panorama nuancé souligne à la fois l’engouement pour les nouvelles technologies et les tensions qu’elles suscitent dans la pratique scientifique.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<p>L’émergence de l’IA générative bouleverse les pratiques de recherche en sciences sociales. De ChatGPT à Gemini, ces outils promettent des gains de productivité, mais soulèvent aussi des questions profondes sur la qualité, l’intégrité et l’éthique de la recherche. Comment les chercheurs s’adaptent-ils à ces nouveaux outils ? Quelles craintes expriment-ils ? Et que nous disent ces tensions sur l’évolution même des sciences sociales ? L’IA ne se contente pas de changer les instruments à disposition : elle transforme les paradigmes épistémologiques et les rapports à la connaissance. Elle oblige les chercheurs à redéfinir les frontières entre l’humain et la machine, entre méthode rigoureuse et automatisation des tâches, entre créativité humaine et synthèse algorithmique.</p>
</section>
<section id="concepts-clés-ia-ml-et-genai" class="level2">
<h2 class="anchored" data-anchor-id="concepts-clés-ia-ml-et-genai">2. Concepts clés : IA, ML et genAI</h2>
<p>L’intelligence artificielle (IA) désigne l’ensemble des systèmes capables d’effectuer des tâches traditionnellement réservées aux humains. Dans le contexte de la recherche, ce terme est souvent associé à des outils génératifs comme ChatGPT ou Gemini, capables de produire du texte, de répondre à des questions ou d’automatiser des segments de l’analyse scientifique. L’apprentissage machine (ML), en revanche, est une sous-catégorie de l’IA qui repose sur des algorithmes permettant à une machine d’apprendre à partir de données. Il est généralement utilisé pour la classification, la prédiction et l’analyse statistique, et perçu comme plus rigoureux, transparent et contrôlable. Enfin, l’IA générative (genAI) désigne les modèles comme les grands modèles de langage (LLM), conçus pour générer du contenu original à partir d’instructions simples, mais dont les mécanismes internes restent souvent opaques.</p>
<p>Les chercheurs interrogés distinguent assez clairement le ML, qu’ils considèrent comme une méthode scientifique parmi d’autres, de l’IA générative, perçue comme une interface technologique accessible mais difficile à maîtriser en profondeur. Cette distinction influence à la fois les usages et le niveau de confiance qu’ils accordent à ces outils.</p>
</section>
<section id="démarche-de-létude" class="level2">
<h2 class="anchored" data-anchor-id="démarche-de-létude">3. Démarche de l’étude</h2>
<p>L’étude repose sur une approche mixte combinant une enquête par questionnaire (N = 284) et des entretiens semi-dirigés (N = 15). Le questionnaire, distribué aléatoirement en deux versions quasi-identiques, posait les mêmes questions, mais remplaçait systématiquement le mot « intelligence artificielle » par « apprentissage machine » pour la moitié des répondants. Cette variation permet de mesurer les effets de perception liés aux termes utilisés.</p>
<p>Les participants provenaient de différentes disciplines des sciences sociales :</p>
<ul>
<li>Psychologie</li>
<li>Sociologie</li>
<li>Économie</li>
<li>Science politique</li>
<li>Éducation</li>
<li>Marketing</li>
</ul>
<p>Ils incluaient à la fois des doctorants, des professeurs, des postdoctorants et des chercheurs associés. Les analyses combinent statistiques descriptives, visualisations exploratoires et codage thématique des verbatim. Cette démarche permet d’explorer tant les usages que les représentations de ces technologies dans le champ académique.</p>
</section>
<section id="résultats-principaux" class="level2">
<h2 class="anchored" data-anchor-id="résultats-principaux">4. Résultats principaux</h2>
<p>La majorité des répondants (73 %) rapportent avoir utilisé un outil d’intelligence artificielle dans leur travail, mais il est crucial de distinguer les types d’IA mobilisés.</p>
<p><strong>IA générative (genAI)</strong> :</p>
<ul>
<li>Rédaction préliminaire d’articles (utilisée par 47 % des répondants au volet IA)</li>
<li>Résumé de littérature (41 %)</li>
<li>Reformulation d’idées (39 %)</li>
<li>Correction grammaticale et stylistique (34 %)</li>
<li>Exploration de domaines nouveaux (28 %)</li>
</ul>
<p><strong>Apprentissage machine (ML)</strong> :</p>
<ul>
<li>Annotation de corpus textuels (29 % des répondants au volet ML)</li>
<li>Classification et prédiction à partir de données (42 %)</li>
<li>Structuration automatisée de grandes bases (25 %)</li>
</ul>
<p><strong>Outils hybrides ou semi-automatisés</strong> :</p>
<ul>
<li>Débogage de code (R, Python) (22 % des répondants dans les deux groupes)</li>
<li>Génération de visualisations (18 %)</li>
<li>Structuration de revues systématiques (15 %)</li>
</ul>
<p>Ces usages montrent une diversité d’approches où les chercheurs adaptent l’outil à la tâche, selon leur niveau de compétence technique et leur besoin de contrôle sur les résultats. L’usage de l’IA, sous toutes ses formes, est particulièrement répandu chez les doctorants, qui manifestent également des niveaux d’acceptation plus élevés que leurs collègues plus avancés dans la carrière.</p>
<p>En revanche, le ML reste perçu comme plus fiable que l’IA générative, notamment en raison de sa proximité avec les méthodes statistiques traditionnelles. L’IA générative, bien qu’attractive pour sa rapidité et son accessibilité, suscite davantage de méfiance quant à la qualité, la vérifiabilité et l’éthique de ses productions. Cette tension entre accessibilité et fiabilité traverse l’ensemble des résultats de l’étude.</p>
</section>
<section id="enjeux-éthiques-et-méthodologiques" class="level2">
<h2 class="anchored" data-anchor-id="enjeux-éthiques-et-méthodologiques">5. Enjeux éthiques et méthodologiques</h2>
<p>L’enthousiasme des chercheurs est souvent tempéré par des inquiétudes bien fondées. Voici les principales préoccupations exprimées :</p>
<ul>
<li><strong>Biais algorithmiques</strong> : mentionnés dans 62 % des réponses au volet IA, les chercheurs craignent que les modèles d’IA perpétuent ou amplifient les inégalités sociales présentes dans les données d’entraînement.</li>
<li><strong>Opaqueness des modèles</strong> : soulevée par 55 % des participants, cette inquiétude touche principalement les grands modèles de langage non open-source, perçus comme difficilement auditables.</li>
<li><strong>Déqualification</strong> : 48 % des répondants expriment la crainte que l’usage intensif de l’IA chez les étudiants diminue leur capacité à raisonner, à apprendre les méthodes ou à interpréter les données.</li>
<li><strong>Manque de normes</strong> : évoqué dans 45 % des réponses, ce point concerne l’absence de cadres clairs sur la manière de documenter, citer ou valider les usages d’IA dans les publications.</li>
<li><strong>Impact environnemental</strong> : signalé par 29 % des participants, le coût énergétique élevé de l’entraînement et du déploiement des modèles est vu comme un enjeu éthique encore trop peu discuté dans les milieux académiques.</li>
</ul>
<p>Malgré ces défis, les chercheurs affirment la nécessité de maintenir le jugement humain au cœur du processus scientifique. L’interprétation des résultats, le développement de théories, l’évaluation de la pertinence sociale ou éthique d’une analyse ne peuvent, selon eux, être automatisés sans perte de sens.</p>
</section>
<section id="recommandations-pour-une-intégration-responsable" class="level2">
<h2 class="anchored" data-anchor-id="recommandations-pour-une-intégration-responsable">6. Recommandations pour une intégration responsable</h2>
<p>Alors que l’usage de l’IA se généralise dans la recherche en sciences sociales, cette section propose des pistes concrètes pour accompagner une intégration responsable et durable. Les recommandations qui suivent s’adressent à différents acteurs – concepteurs de technologies, chercheurs, institutions éducatives et décideurs – et reflètent les propositions formulées par les participants à l’étude. Elles visent à favoriser la transparence, la réflexivité, l’équité et l’ancrage éthique des pratiques scientifiques assistées par IA.</p>
<p><strong>Pour les développeurs :</strong></p>
<ul>
<li>Créer des outils interprétables, traçables et documentés, afin de permettre aux utilisateurs de comprendre et d’évaluer les processus automatisés.</li>
<li>Rendre visibles les jeux de données, les sources et les limites d’usage, pour assurer la traçabilité et prévenir les biais implicites.</li>
</ul>
<p><strong>Pour les chercheurs :</strong></p>
<ul>
<li>Développer une littératie critique de l’IA, incluant une compréhension de ses forces, limites et implications sociales.</li>
<li>Documenter et valider les usages dans leurs publications, en expliquant comment et pourquoi un outil a été utilisé dans le cadre de la recherche.</li>
</ul>
<p><strong>Pour les établissements d’enseignement :</strong></p>
<ul>
<li>Intégrer les enjeux sociotechniques de l’IA dans les cursus, notamment par des modules transversaux accessibles à toutes les disciplines.</li>
<li>Soutenir les initiatives interdisciplinaires qui favorisent une approche réflexive, collective et expérimentale de l’usage de l’IA.</li>
</ul>
<p><strong>Pour les décideurs publics :</strong></p>
<ul>
<li>Garantir un accès équitable aux outils, notamment en soutenant des solutions open source ou accessibles pour les institutions moins dotées.</li>
<li>Exiger des pratiques durables en matière de développement technologique, incluant des critères environnementaux et sociaux.</li>
<li>Encourager une gouvernance participative impliquant les chercheurs, les communautés et les utilisateurs finaux dans les orientations stratégiques de l’IA en recherche.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">7. Conclusion</h2>
<p>Cette étude brosse un portrait lucide et richement documenté des rapports que les sciences sociales entretiennent avec l’intelligence artificielle. Loin d’un rejet ou d’un engouement naïf, les chercheurs interrogés montrent une volonté claire de réfléchir collectivement aux conditions d’une utilisation responsable de ces technologies. Ils rappellent que les outils numériques ne sont pas neutres : ils véhiculent des valeurs, des biais, des modèles de pensée. Leur intégration dans la recherche doit donc faire l’objet d’un débat rigoureux et ouvert. Plus qu’une simple question d’efficacité, l’IA pose un défi fondamental à la manière dont nous produisons, validons et partageons le savoir en société.</p>
</section>
<section id="référence" class="level2">
<h2 class="anchored" data-anchor-id="référence">8. Référence</h2>
<p>Chakravorti, T., Wang, X., Venkit, P. N., Koneru, S., Munger, K., &amp; Rajtmajer, S. (2025). <em>Social Scientists on the Role of AI in Research</em>. arXiv preprint https://arxiv.org/abs/2506.11255</p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>IA et recherche universitaire</category>
  <guid>https://benoitplante.github.io/posts/2025-06-19-AI-sciences-sociales/</guid>
  <pubDate>Thu, 19 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-19-AI-sciences-sociales/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>L’intelligence artificielle transforme-t-elle la recherche en sciences sociales ?</title>
  <dc:creator>Benoit Plante + ChatGPT</dc:creator>
  <link>https://benoitplante.github.io/posts/2025-06-16-ML-sciences-sociales/</link>
  <description><![CDATA[ 





<section id="générée-par-chatgpt" class="level2">
<h2 class="anchored" data-anchor-id="générée-par-chatgpt"><img src="https://benoitplante.github.io/posts/2025-06-16-ML-sciences-sociales/banner.png" class="img-fluid quarto-figure quarto-figure-center" style="width:100.0%" alt="Générée par ChatGPT"></h2>
</section>
<section id="lintelligence-artificielle-transforme-t-elle-la-recherche-en-sciences-sociales" class="level2">
<h2 class="anchored" data-anchor-id="lintelligence-artificielle-transforme-t-elle-la-recherche-en-sciences-sociales">L’intelligence artificielle transforme-t-elle la recherche en sciences sociales ?</h2>
<p><strong>En bref</strong><br>
Une revue méthodique publiée dans <em>IEEE Access</em> démontre comment les techniques d’intelligence artificielle, notamment l’apprentissage automatique (ML), le traitement du langage naturel (NLP) et les modèles explicables (XAI), redéfinissent en profondeur les façons de faire en sciences sociales. Ces outils permettent non seulement d’explorer des données massives et variées, mais aussi de développer des modèles prédictifs, de tester des hypothèses causales complexes, d’évaluer des politiques publiques et de détecter des dynamiques sociales invisibles autrement. En parallèle, les auteurs insistent sur les risques de biais, d’opacité algorithmique et de dérives éthiques, plaidant pour une gouvernance responsable de l’IA appliquée aux sciences humaines.</p>
</section>
<section id="pourquoi-sintéresser-à-lia-dans-les-sciences-sociales" class="level2">
<h2 class="anchored" data-anchor-id="pourquoi-sintéresser-à-lia-dans-les-sciences-sociales">1. Pourquoi s’intéresser à l’IA dans les sciences sociales ?</h2>
<p>Pour Elias Dritsas et Maria Trigka, le recours à l’intelligence artificielle ne constitue pas un simple changement d’outils, mais bien un changement de paradigme. L’IA offre aux chercheurs une puissance analytique sans précédent pour traiter des volumes massifs de données complexes, hétérogènes, souvent non structurées. Elle permet d’explorer des comportements individuels et collectifs en temps réel, de modéliser des phénomènes dynamiques et d’extraire des régularités sans imposer a priori de cadre théorique rigide.</p>
<p>Les auteurs insistent aussi sur la nécessité d’intégrer les apports de l’IA dans des cadres théoriques solides, car la seule puissance computationnelle ne suffit pas. Ce dialogue entre modèles prédictifs inductifs et raisonnements explicatifs déductifs constitue, selon eux, une voie féconde pour renouveler l’épistémologie des sciences sociales. En somme, l’IA ne remplace pas la réflexion critique, mais elle la stimule, en rendant visibles des motifs complexes qui seraient restés inaccessibles autrement.</p>
</section>
<section id="explorer-les-dynamiques-sociales-par-apprentissage-machine" class="level2">
<h2 class="anchored" data-anchor-id="explorer-les-dynamiques-sociales-par-apprentissage-machine">2. Explorer les dynamiques sociales par apprentissage machine</h2>
<p>L’apprentissage automatique, ou <em>machine learning</em>, regroupe différentes méthodes dont le fonctionnement peut être illustré par des analogies concrètes. Par exemple, l’apprentissage supervisé fonctionne comme un professeur qui enseigne à un élève à reconnaître des formes : on fournit au modèle des exemples déjà étiquetés (par exemple, des individus dont on connaît l’orientation politique ou le niveau d’anxiété), et il apprend à reconnaître des régularités dans les données pour faire des prédictions sur de nouveaux cas.</p>
<p>Dans les sciences sociales, cela permet de prédire des comportements comme l’intention de vote, la probabilité de décrochage scolaire ou encore la détresse psychologique à partir d’une combinaison de variables (textes, réponses à des questionnaires, historique d’activité numérique, etc.). Des modèles comme les forêts aléatoires ou les machines à vecteurs de support sont souvent utilisés pour ce type de tâche. Leur grande force réside dans leur capacité à intégrer de nombreuses variables et à modéliser des relations non linéaires, là où les modèles classiques de régression peuvent s’avérer limités.</p>
<p>L’apprentissage non supervisé, lui, s’apparente davantage à une exploration : on ne donne pas d’étiquette au modèle, mais on le laisse repérer des regroupements spontanés ou des structures cachées dans les données. Par exemple, il peut détecter différents profils de participants dans une enquête d’opinion ou faire ressortir des segments idéologiques sur Twitter sans avoir à prédéfinir ces catégories. Des méthodes comme le <em>k-means</em>, la classification hiérarchique ou la réduction de dimension (PCA, UMAP) sont employées ici pour simplifier la complexité et favoriser l’interprétation.</p>
<p>Enfin, l’apprentissage profond (<em>deep learning</em>), basé sur des réseaux de neurones inspirés du fonctionnement du cerveau humain, est particulièrement efficace pour traiter des données complexes comme les textes, les images ou les séries temporelles. Un modèle de type LSTM (Long Short-Term Memory), par exemple, est capable de modéliser les évolutions dans le temps — comme l’évolution de l’état émotionnel d’une personne à travers ses messages au fil des jours. Ces modèles sont également adaptés à l’analyse de journaux cliniques, de flux de messages ou de trajectoires éducatives.</p>
</section>
<section id="le-langage-naturel-comme-terrain-danalyse-central" class="level2">
<h2 class="anchored" data-anchor-id="le-langage-naturel-comme-terrain-danalyse-central">3. Le langage naturel comme terrain d’analyse central</h2>
<p>Le traitement du langage naturel (<em>natural language processing</em>, NLP) permet à l’IA d’analyser des textes à grande échelle. Imaginez que vous deviez lire des milliers de discours politiques, de messages sur les réseaux sociaux ou de décisions judiciaires : le NLP rend cette tâche possible, non seulement en lisant les textes, mais en identifiant des sentiments, des thèmes ou des prises de position implicites.</p>
<p>Par exemple, l’analyse de sentiment utilise des modèles (parfois aussi simples que des dictionnaires d’émotions, parfois aussi puissants que des modèles comme BERT ou GPT) pour classer des phrases comme étant positives, négatives ou neutres. Cela permet d’évaluer l’impact émotionnel d’un événement politique, ou encore de détecter les signaux d’anxiété ou de colère dans des messages publiés sur les médias sociaux. Les chercheurs peuvent ainsi surveiller l’évolution du climat émotionnel dans une population donnée.</p>
<p>Le <em>topic modeling</em>, quant à lui, utilise des algorithmes comme LDA (Latent Dirichlet Allocation) pour identifier les sujets dominants dans un corpus de textes. Dans une campagne électorale, on peut ainsi voir quels thèmes (ex. : immigration, économie, santé) sont les plus discutés, et comment leur importance varie dans le temps. Combiné à une analyse temporelle ou spatiale, ce type de modèle devient un véritable outil de veille politique ou médiatique.</p>
</section>
<section id="lanalyse-de-réseaux-vers-une-cartographie-structurelle-des-relations-sociales" class="level2">
<h2 class="anchored" data-anchor-id="lanalyse-de-réseaux-vers-une-cartographie-structurelle-des-relations-sociales">4. L’analyse de réseaux : vers une cartographie structurelle des relations sociales</h2>
<p>Les graphes sont des représentations de relations entre entités. En sciences sociales, cela peut servir à représenter des interactions entre personnes, entre groupes, ou entre idées. Un nœud est une entité (par exemple une personne), et une arête est une relation (par exemple une amitié, un échange ou une co-citation).</p>
<p>Analyser ces réseaux permet de détecter des structures comme des communautés (groupes soudés), des individus très influents (nœuds centraux), ou encore des ponts entre groupes séparés. Ces éléments sont cruciaux pour comprendre la circulation de l’information, la formation des coalitions politiques ou l’émergence des controverses.</p>
<p>Les chercheurs utilisent aussi des modèles plus avancés, appelés réseaux neuronaux de graphes (Graph Neural Networks), qui combinent les propriétés des nœuds avec celles de leur environnement pour faire des prédictions. Cela peut servir à prévoir quels individus sont les plus susceptibles de diffuser une information, ou quels groupes risquent de se radicaliser dans des forums en ligne. Ces approches, qui marient modélisation structurelle et apprentissage automatique, offrent une puissance inédite pour anticiper les dynamiques sociales.</p>
</section>
<section id="inférer-des-causalités-dans-des-systèmes-complexes" class="level2">
<h2 class="anchored" data-anchor-id="inférer-des-causalités-dans-des-systèmes-complexes">5. Inférer des causalités dans des systèmes complexes</h2>
<p>Prédire un phénomène est utile, mais comprendre <em>pourquoi</em> il se produit est fondamental. L’inférence causale cherche à répondre à la question : que se passerait-il si… ? Par exemple, que se passerait-il si l’on augmentait les prestations sociales dans une région donnée ?</p>
<p>Pour répondre à cela, on peut utiliser des outils comme les modèles à forêts causales, qui permettent d’estimer l’effet d’une intervention sur différents sous-groupes (ex. : selon l’âge ou le niveau de scolarité). Ces approches permettent de détecter les hétérogénéités de traitement, c’est-à-dire de savoir pour qui une intervention est plus efficace.</p>
<p>Des modèles contrefactuels vont plus loin en simulant ce qui se serait passé dans un scénario alternatif. Ces outils sont essentiels pour guider les politiques publiques ou évaluer les effets de programmes sociaux. Par exemple, un modèle contrefactuel pourrait estimer les conséquences d’un changement de politique éducative sur la persévérance scolaire, en tenant compte des différences régionales et sociodémographiques.</p>
<p>Mais ces approches nécessitent de poser des hypothèses claires, et de s’assurer que les données sont suffisamment riches pour représenter les mécanismes étudiés. Sinon, le modèle pourrait attribuer un effet causal à une simple coïncidence. Il est donc crucial d’allier rigueur statistique et connaissance fine du terrain.</p>
</section>
<section id="pour-une-ia-éthique-et-contextualisée-en-sciences-sociales" class="level2">
<h2 class="anchored" data-anchor-id="pour-une-ia-éthique-et-contextualisée-en-sciences-sociales">6. Pour une IA éthique et contextualisée en sciences sociales</h2>
<p>Un des grands mérites de l’article est de ne pas esquiver les questions éthiques. Les auteurs rappellent qu’un algorithme n’est pas neutre : il reflète les données sur lesquelles il a été entraîné, et celles-ci sont souvent marquées par les inégalités sociales passées. Un modèle de prédiction du risque de récidive, par exemple, pourrait reproduire les biais raciaux contenus dans les bases de données policières.</p>
<p>Pour éviter cela, plusieurs solutions existent : utiliser des modèles explicables (comme les arbres de décision), tester la robustesse des résultats, intégrer des indicateurs d’équité, ou encore impliquer les citoyens et les groupes concernés dans le développement des outils. L’IA, surtout en contexte social, ne peut être conçue sans dialogue.</p>
<p>Les auteurs soulignent également l’importance d’un encadrement institutionnel : chartes éthiques, audits algorithmiques, comités de validation, normes de transparence. L’intégration de ces principes dans la pratique scientifique est une condition essentielle pour que l’IA contribue réellement à une société plus juste.</p>
</section>
<section id="conclusion-pour-une-science-sociale-augmentée-critique-et-engagée" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-pour-une-science-sociale-augmentée-critique-et-engagée">7. Conclusion : pour une science sociale augmentée, critique et engagée</h2>
<p>En fin de compte, les auteurs ne proposent pas de remplacer les méthodes traditionnelles, mais de les enrichir. L’IA offre un accès inédit à des données massives, des moyens d’analyse puissants et des possibilités de simulation très prometteuses. Mais ces outils doivent rester au service d’une compréhension humaine des phénomènes sociaux : contextualisée, critique, et orientée vers le bien commun.</p>
<p>Pour cela, il faut encourager les collaborations interdisciplinaires, investir dans la formation des chercheurs aux méthodes computationnelles, et développer une culture de l’explicabilité et de la réflexivité. L’enjeu n’est pas de faire de tous les sociologues des programmeurs, mais de leur permettre de dialoguer avec les ingénieurs, et vice-versa.</p>
</section>
<section id="référence-complète" class="level2">
<h2 class="anchored" data-anchor-id="référence-complète">Référence complète</h2>
<p>Dritsas, E., &amp; Trigka, M. (2025). <em>Machine Learning and Data Science in Social Sciences: Methods, Applications, and Future Directions</em>. IEEE Access. https://doi.org/10.1109/ACCESS.2025.3578906</p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>Méthodologie augmentée par l’IA</category>
  <guid>https://benoitplante.github.io/posts/2025-06-16-ML-sciences-sociales/</guid>
  <pubDate>Mon, 16 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-16-ML-sciences-sociales/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>IAGen dans l’enseignement supérieur : ce que pensent vraiment les étudiants</title>
  <dc:creator>Benoit Plante + ChatGPT</dc:creator>
  <link>https://benoitplante.github.io/posts/2025-06-13-AIGen-etudiant-1/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-13-AIGen-etudiant-1/banner.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<p><strong>En bref</strong> : Une vaste étude menée auprès de 460 étudiants d’enseignement supérieur révèle que la majorité utilise régulièrement l’intelligence artificielle générative (IAGen), notamment ChatGPT, et se sent relativement à l’aise avec ces outils. Les étudiants y voient des leviers de productivité et d’apprentissage, mais formulent aussi des préoccupations quant à l’intégrité académique, à la fiabilité des contenus et à la confidentialité des données. Les perceptions varient selon le type d’établissement, le statut d’emploi et le niveau d’études, ce qui souligne la nécessité d’un encadrement pédagogique et éthique de l’usage de l’IAGen dans les cursus universitaires.</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>L’apparition de ChatGPT et la diffusion massive d’outils d’intelligence artificielle générative (IAGen) marquent un tournant dans l’enseignement supérieur. En peu de temps, ces technologies sont passées d’objets de curiosité à des instruments utilisés quotidiennement par des milliers d’étudiants. Mais qu’en pensent réellement les principaux concernés ? L’étude de Maxwell et coll. (2025) apporte un éclairage rigoureux et nuancé sur les usages, les attentes, les réticences et les besoins exprimés par les étudiants face à IAGen dans leurs parcours académiques.</p>
</section>
<section id="les-dimensions-psychologiques-mobilisées" class="level2">
<h2 class="anchored" data-anchor-id="les-dimensions-psychologiques-mobilisées">Les dimensions psychologiques mobilisées</h2>
<p>Pour comprendre l’adoption des outils IAGen, les auteurs s’appuient sur des modèles éprouvés en psychologie et en sciences des technologies éducatives. La théorie de la valeur expectative (Wigfield &amp; Eccles, 2000) explique que l’engagement dépend à la fois de l’utilité perçue d’un outil et du coût anticipé de son usage. Le modèle d’acceptation technologique (TAM) met l’accent sur l’utilité perçue et la facilité d’utilisation. Enfin, la théorie unifiée d’acceptation des technologies (UTAUT) souligne le rôle de la pression sociale, de la performance attendue et de la confiance dans l’adoption technologique. Ces cadres permettent de structurer l’analyse des perceptions étudiantes de IAGen selon des dimensions de valeur, de compétence, d’éthique et de contexte.</p>
</section>
<section id="démarche-de-létude" class="level2">
<h2 class="anchored" data-anchor-id="démarche-de-létude">Démarche de l’étude</h2>
<p>L’enquête a été réalisée au printemps 2024 auprès de 460 étudiants inscrits dans divers établissements d’enseignement supérieur aux États-Unis. Le questionnaire couvrait plusieurs dimensions : fréquence d’utilisation de IAGen, niveaux de confort et de compétence, perception des bénéfices et des risques, et caractéristiques sociodémographiques. Les instruments utilisés ont été adaptés d’échelles validées et ont fait l’objet d’une analyse factorielle confirmatoire. Trois grands axes ont été mesurés :</p>
<ul>
<li><strong>Préparation</strong> : compréhension, conscience éthique, compétences d’utilisation ;</li>
<li><strong>Bénéfices</strong> : efficacité perçue et sentiment d’autonomisation ;</li>
<li><strong>Défis</strong> : préoccupations éthiques, limitations techniques, impact éducatif négatif.</li>
</ul>
<p>Les analyses statistiques ont permis de dégager des tendances générales ainsi que des variations selon le genre, l’âge, le niveau d’études, le statut d’emploi, le nombre de cours en ligne suivis et le type d’institution.</p>
</section>
<section id="principaux-résultats" class="level2">
<h2 class="anchored" data-anchor-id="principaux-résultats">Principaux résultats</h2>
<p>L’étude met en évidence plusieurs constats clés :</p>
<ul>
<li><strong>Une adoption massive</strong> : 84 % des étudiants déclarent utiliser IAGen à des fins éducatives, souvent pour obtenir des réponses à des questions générales ou techniques, ou encore pour rédiger des documents.</li>
<li><strong>Un rapport globalement positif</strong> : près de 80 % des répondants se disent à l’aise avec ces outils, appréciant leur disponibilité, leur accessibilité, et surtout l’absence de jugement perçu.</li>
<li><strong>Des bénéfices bien identifiés</strong> : les étudiants soulignent les gains en productivité, la possibilité d’un feedback immédiat, et l’utilité pour structurer leurs idées ou clarifier des concepts.</li>
<li><strong>Des limites bien reconnues</strong> : les répondants expriment des préoccupations sur la fiabilité des réponses, les enjeux de plagiat, la perte d’esprit critique et les risques d’une dépendance cognitive aux outils.</li>
</ul>
</section>
<section id="différences-démographiques-importantes" class="level2">
<h2 class="anchored" data-anchor-id="différences-démographiques-importantes">Différences démographiques importantes</h2>
<p>Certaines différences marquantes sont observées :</p>
<ul>
<li>Les <strong>étudiants de collèges techniques ou communautaires</strong> montrent une plus grande aisance et des attitudes plus favorables envers IAGen, possiblement en lien avec des curriculums plus orientés vers les compétences pratiques et technologiques.</li>
<li>Les <strong>étudiants sans emploi ou à temps partiel</strong> perçoivent davantage de bénéfices que leurs homologues en emploi à temps plein, peut-être en raison d’un temps disponible plus grand pour expérimenter.</li>
<li>Les <strong>étudiants de premier cycle</strong> (en particulier les deuxièmes années) ont une vision plus enthousiaste de IAGen, alors que les étudiants aux cycles supérieurs (maîtrise, doctorat) sont plus critiques et conscients des limitations.</li>
</ul>
</section>
<section id="enjeux-pour-la-formation-universitaire" class="level2">
<h2 class="anchored" data-anchor-id="enjeux-pour-la-formation-universitaire">Enjeux pour la formation universitaire</h2>
<p>Face à l’usage croissant de IAGen, l’étude souligne l’importance d’intégrer une formation structurée à ces outils dans les parcours académiques. Cela inclut :</p>
<ul>
<li>Une <strong>littératie de l’IA</strong> qui aborde les usages, les limites, et les dilemmes éthiques ;</li>
<li>Des <strong>activités pédagogiques</strong> qui incitent à la réflexion critique sur les contenus générés par IA ;</li>
<li>Des <strong>ressources différenciées</strong> selon le niveau, le contexte et les besoins des étudiants.</li>
</ul>
<p>Les établissements sont appelés à jouer un rôle actif dans l’encadrement de ces usages, tout en tenant compte de la diversité des profils étudiants.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>L’étude de Maxwell et coll. (2025) constitue un apport important pour comprendre la place croissante de IAGen dans l’enseignement supérieur. En révélant à la fois l’enthousiasme, la curiosité et les inquiétudes des étudiants, elle invite les acteurs du milieu universitaire à dépasser les discours polarisés pour construire des environnements d’apprentissage où IAGen est utilisé de façon critique, éthique et inclusive. Ce travail rappelle également que la technologie ne remplace pas l’humain : elle redéfinit les conditions de l’apprentissage, et c’est à nous de les accompagner avec lucidité.</p>
</section>
<section id="référence" class="level2">
<h2 class="anchored" data-anchor-id="référence">Référence</h2>
<p>Maxwell, D., Oyarzun, B., Kim, S., &amp; Bong, J. Y. (2025). Generative AI in Higher Education: Demographic Differences in Student Perceived Readiness, Benefits, and Challenges. TechTrends. https://doi.org/10.1007/s11528-025-01109-6</p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>IA et éducation</category>
  <guid>https://benoitplante.github.io/posts/2025-06-13-AIGen-etudiant-1/</guid>
  <pubDate>Fri, 13 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-13-AIGen-etudiant-1/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>L’IA peut-elle mieux prédire les corrélations en personnalité que les experts ?</title>
  <link>https://benoitplante.github.io/posts/2025-06-09-AI-correlation/</link>
  <description><![CDATA[ 





<p><strong>En bref</strong>: Une étude récente montre que les grands modèles de langage (LLMs) et les systèmes spécialisés peuvent surpasser les humains – y compris les experts en psychologie – dans la prédiction des corrélations entre items de questionnaires de personnalité. Ces résultats soulèvent des questions importantes pour la recherche psychométrique, les pratiques d’évaluation et les usages futurs de l’IA en psychologie. Ils suggèrent également de nouvelles avenues pour l’intégration de l’IA dans le processus scientifique, notamment dans les phases exploratoires de formulation d’hypothèses.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-09-AI-correlation/banner.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1. Introduction</h2>
<p>Dans l’évaluation psychologique, la corrélation entre les items d’un questionnaire est une information clé pour comprendre les traits de personnalité, construire des échelles valides et tester des hypothèses. C’est une pratique courante dans le développement d’instruments psychométriques, qui permet de vérifier si plusieurs énoncés mesurent bien un même construit psychologique.</p>
<p>Mais que se passerait-il si des modèles d’intelligence artificielle, sans vécu ni subjectivité humaine, étaient capables de mieux prédire ces corrélations que nous ? Peut-on réellement remplacer, ou du moins égaler, l’intuition experte d’un chercheur en psychologie avec une machine ?</p>
<p>C’est la question qu’ont explorée Philipp Schoenegger et ses collègues (2025) en comparant la performance de différents systèmes d’IA, d’étudiants gradués et de professeurs en psychologie, ainsi que de non-experts dans une tâche de prédiction de corrélations entre items de personnalité. Leurs résultats bousculent certaines idées reçues sur les limites actuelles de l’intelligence artificielle dans les sciences humaines.</p>
</section>
<section id="concepts-de-base" class="level2">
<h2 class="anchored" data-anchor-id="concepts-de-base">2. Concepts de base</h2>
<p>L’étude repose sur un concept central en psychométrie : <strong>la corrélation entre items</strong>, c’est-à-dire le degré de relation entre les réponses données à deux énoncés différents d’un questionnaire (par exemple : « Je suis souvent anxieux » et « Je me fais du souci pour des détails »). Plus la corrélation est forte, plus ces items ont de chances de mesurer un aspect commun de la personnalité.</p>
<p>Ces corrélations permettent non seulement de construire des échelles cohérentes et fiables, mais aussi de détecter des regroupements de traits (comme les Big Five ou la triade sombre), d’établir des profils psychologiques et d’analyser les interrelations entre différentes dimensions de la personnalité humaine. Elles jouent un rôle essentiel dans la validation d’outils, la modélisation de concepts et la formulation d’hypothèses empiriques.</p>
</section>
<section id="contexte-technologique" class="level2">
<h2 class="anchored" data-anchor-id="contexte-technologique">3. Contexte technologique</h2>
<p>Deux grandes catégories de modèles d’IA ont été testées :</p>
<ul>
<li><strong>Modèles généralistes</strong> : GPT-4o et Claude 3 Opus, deux modèles de langage de pointe capables de traiter du texte, de répondre à une vaste gamme de questions et de s’adapter à des domaines variés.</li>
<li><strong>Modèle spécialisé</strong> : <em>PersonalityMap</em>, un réseau de neurones profond entraîné spécifiquement sur un grand corpus de données psychométriques pour prédire les corrélations entre items de personnalité.</li>
</ul>
<p>Cette comparaison permet d’évaluer l’apport des systèmes généralistes, polyvalents mais non spécialisés, par rapport aux modèles conçus pour une tâche psychologique précise. Le défi est de déterminer si la puissance de généralisation des LLMs peut rivaliser avec l’optimisation ciblée d’un modèle comme PersonalityMap dans un domaine aussi nuancé que la psychologie des traits.</p>
</section>
<section id="démarche-de-létude" class="level2">
<h2 class="anchored" data-anchor-id="démarche-de-létude">4. Démarche de l’étude</h2>
<p>Les auteurs ont demandé à cinq groupes de produire des estimations de corrélations entre 249 paires d’items tirées de l’inventaire SAPA de personnalité :</p>
<ul>
<li>254 personnes non expertes (recrutées via Positly)</li>
<li>272 experts (doctorant·es, postdoctorant·es et professeur·es en psychologie)</li>
<li>Les modèles GPT-4o et Claude 3 Opus (3 prédictions chacune, à température 0)</li>
<li>Le modèle spécialisé PersonalityMap (1 prédiction par paire, déterministe)</li>
</ul>
<p>Les réponses humaines étaient saisies sur une échelle continue de -1 à +1. Une brève formation sur les corrélations était offerte aux participants. Les performances ont ensuite été comparées selon plusieurs indicateurs : erreur absolue moyenne (précision), corrélation avec les données empiriques (finesse relative) et capacité à classer les corrélations dans les bonnes catégories directionnelles (positives, nulles ou négatives).</p>
</section>
<section id="résultats" class="level2">
<h2 class="anchored" data-anchor-id="résultats">5. Résultats</h2>
<p>Les résultats sont impressionnants et sans équivoque :</p>
<ul>
<li><strong>PersonalityMap</strong> obtient l’erreur moyenne la plus faible (0.07), suivie de Claude 3 Opus (0.11), GPT-4o (0.14), les experts (0.20) et les non-experts (0.29).</li>
<li>Dans 99 % des comparaisons individuelles, PersonalityMap prédit mieux que les experts humains.</li>
<li>Lorsqu’on regroupe les réponses humaines (effet de foule), les experts égalent la performance du modèle spécialisé, soulignant l’intérêt des prédictions collectives.</li>
<li>Les modèles généralistes surpassent la majorité des individus, experts inclus, dans les prédictions non agrégées.</li>
</ul>
<p>Les auteurs notent que l’avantage des IA tient notamment à leur calibration : elles prédisent des magnitudes proches des véritables corrélations observées, tandis que les humains tendent à surestimer ou sous-estimer les relations. Toutefois, les experts conservent un léger avantage pour identifier la direction (positive ou négative) des relations entre items.</p>
</section>
<section id="portée-et-utilité" class="level2">
<h2 class="anchored" data-anchor-id="portée-et-utilité">6. Portée et utilité</h2>
<p>Ces résultats sont prometteurs pour plusieurs champs :</p>
<ul>
<li><strong>La recherche psychométrique</strong> : tester des hypothèses rapidement, construire ou affiner des instruments, détecter des regroupements d’items pertinents.</li>
<li><strong>Les pratiques appliquées</strong> : pré-identifier des relations utiles entre comportements ou traits, sans recourir à de vastes échantillons, en particulier dans des contextes cliniques ou organisationnels.</li>
<li><strong>Le développement d’outils</strong> : concevoir des systèmes d’assistance à la modélisation de questionnaires, à la génération d’items ou à la simulation d’effets, avec des cycles d’itération rapides.</li>
</ul>
<p>L’étude suggère même que ces modèles pourraient servir de « tubes à essai numériques » pour la recherche en psychologie, permettant d’explorer des hypothèses à bas coût avant la collecte de données réelles.</p>
</section>
<section id="limites-et-perspectives" class="level2">
<h2 class="anchored" data-anchor-id="limites-et-perspectives">7. Limites et perspectives</h2>
<p>L’étude présente plusieurs limites importantes :</p>
<ul>
<li>Le jeu de données repose uniquement sur l’inventaire SAPA, composé d’items auto-rapportés et d’un seul format (cross-sectionnel).</li>
<li>Les performances des LLMs pourraient être biaisées si les données utilisées pour évaluer les modèles faisaient partie de leur corpus d’entraînement.</li>
<li>Les modèles prédisent uniquement des corrélations linéaires et ne permettent pas d’inférence causale, ce qui limite leur usage dans les analyses avancées.</li>
</ul>
<p>Les auteurs recommandent aussi d’évaluer la performance sur d’autres types d’items, d’estimer l’incertitude associée aux prédictions, et d’explorer des relations non linéaires ou modérées. Enfin, la question éthique de l’usage de ces systèmes dans des contextes appliqués reste ouverte.</p>
</section>
<section id="conclusion-personnelle" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-personnelle">8. Conclusion personnelle</h2>
<p>Cette étude marque un tournant pour la psychologie computationnelle. Elle montre que l’IA ne se contente plus de simuler la personnalité ou d’inférer des traits à partir de textes : elle peut désormais prédire finement la structure des relations entre items psychométriques. Ce résultat ouvre la porte à une nouvelle génération d’outils psychométriques assistés par IA.</p>
<p>Mais il pose aussi une question fondamentale : voulons-nous confier à des algorithmes le pouvoir de prédire, voire de diagnostiquer, nos traits les plus intimes ? Une réflexion éthique, méthodologique et épistémologique s’impose. Il faudra aussi penser à la complémentarité entre expertise humaine et intelligence computationnelle, plutôt qu’à leur substitution pure et simple.</p>
</section>
<section id="référence-complète" class="level2">
<h2 class="anchored" data-anchor-id="référence-complète">9. Référence complète</h2>
<p>Schoenegger, P., Greenberg, S., Grishin, A., Lewis, J., &amp; Caviola, L. (2025). AI can outperform humans in predicting correlations between personality items. <em>Communications Psychology, 3</em>(23). <a href="https://doi.org/10.1038/s44271-025-00205-w">https://doi.org/10.1038/s44271-025-00205-w</a></p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>Méthodologie augmentée par l’IA</category>
  <guid>https://benoitplante.github.io/posts/2025-06-09-AI-correlation/</guid>
  <pubDate>Mon, 09 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-09-AI-correlation/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Une nouvelle revue pour penser l’IA autrement</title>
  <link>https://benoitplante.github.io/posts/2025-06-06-Journal_psychology_AI/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-06-Journal_psychology_AI/banner.jpg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="une-nouvelle-revue-pour-penser-lia-autrement" class="level2">
<h2 class="anchored" data-anchor-id="une-nouvelle-revue-pour-penser-lia-autrement">Une nouvelle revue pour penser l’IA autrement</h2>
<p>Le lancement officiel du Journal of Psychology and AI a été annoncé par Taylor &amp; Francis le 3 juillet 2024. Cette nouvelle revue s’inscrit dans une dynamique croissante d’intégration des sciences psychologiques et des technologies intelligentes.</p>
<section id="pourquoi-cette-revue-est-elle-importante" class="level3">
<h3 class="anchored" data-anchor-id="pourquoi-cette-revue-est-elle-importante">Pourquoi cette revue est-elle importante ?</h3>
<p>Alors que les systèmes d’IA envahissent les sphères de la recherche, de la clinique, de l’éducation ou de la vie quotidienne, il devient essentiel de réfléchir aux transformations qu’ils introduisent dans nos façons de penser, de connaître, d’interagir et d’évaluer. Le journal souhaite répondre à une demande croissante pour une compréhension psychologique approfondie des implications de l’IA, tant pour les individus que pour les institutions. Il se positionne également comme une plateforme de réflexion critique sur les nouvelles frontières de la psychologie à l’ère algorithmique.</p>
<p>La revue entend accueillir à la fois des travaux empiriques, théoriques et critiques, dans un esprit d’ouverture disciplinaire. Elle accepte des recherches quantitatives, qualitatives, expérimentales, et des revues systématiques.</p>
</section>
<section id="qui-sont-les-chercheurs-derrière-cette-initiative" class="level3">
<h3 class="anchored" data-anchor-id="qui-sont-les-chercheurs-derrière-cette-initiative">Qui sont les chercheurs derrière cette initiative ?</h3>
<p>Le journal est dirigé par deux chercheurs basés en Nouvelle-Zélande :</p>
<ul>
<li><p><strong>Christian U. Krägeloh, Ph. D.</strong> (Auckland University of Technology). Professeur reconnu pour ses travaux sur le bien-être psychologique, la pleine conscience et les enjeux éthiques liés à l’IA. Il est aussi éditeur fondateur du Journal of Contemplative Studies et a publié sur les usages de la technologie dans le bien-être.</p></li>
<li><p><strong>Oleg N. Medvedev, Ph. D.</strong> (University of Waikato). Spécialiste de méthodologie quantitative, de validation psychométrique et de cognition humaine augmentée. Il est connu pour ses contributions en psychologie de la santé et en recherche appliquée sur l’utilisation de l’IA en contexte clinique.</p></li>
</ul>
<p>Tous deux portent une vision de l’intelligence artificielle comme objet d’étude psychologique autant que comme outil transformateur de la discipline.</p>
</section>
<section id="quelques-thématiques-centrales" class="level3">
<h3 class="anchored" data-anchor-id="quelques-thématiques-centrales">Quelques thématiques centrales</h3>
<ul>
<li>Psychologie des interactions humain-machine</li>
<li>IA générative et modélisation cognitive</li>
<li>Utilisation d’agents intelligents en contexte clinique ou éducatif</li>
<li>Biais algorithmiques, éthique de la simulation et transparence</li>
<li>Conception d’outils adaptatifs en santé mentale</li>
<li>Perception sociale et acceptabilité des technologies intelligentes</li>
<li>Impact des environnements numériques sur le comportement humain</li>
</ul>
</section>
<section id="à-lire-dans-le-premier-numéro" class="level3">
<h3 class="anchored" data-anchor-id="à-lire-dans-le-premier-numéro">À lire dans le premier numéro</h3>
<p>Le premier numéro comprend plusieurs contributions explorant les limites actuelles des modèles de langage et les tensions entre intelligence humaine et artificielle. Pour un résumé d’un des articles, voir le billet :👉 <em><a href="./2025-06-05-intelligence-naturelle-ia">Naturelle ou artificielle ? L’intelligence à l’ère des modèles de langage</a></em></p>
</section>
<section id="pour-qui" class="level3">
<h3 class="anchored" data-anchor-id="pour-qui">Pour qui ?</h3>
<p>Cette revue s’adresse à un public interdisciplinaire :</p>
<ul>
<li>Psychologues cognitivistes, cliniciens, développementalistes ou sociaux</li>
<li>Chercheurs en science des données, IA ou neurosciences</li>
<li>Professionnels de l’intervention utilisant des technologies intelligentes</li>
<li>Philosophe de l’esprit, éthiciennes, sociologues du numérique</li>
</ul>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>
</section>

 ]]></description>
  <category>Psychologie et IA</category>
  <guid>https://benoitplante.github.io/posts/2025-06-06-Journal_psychology_AI/</guid>
  <pubDate>Fri, 06 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-06-Journal_psychology_AI/banner.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Naturelle ou artificielle ? L’intelligence à l’ère des modèles de langage</title>
  <link>https://benoitplante.github.io/posts/2025-06-06-webster-2025/</link>
  <description><![CDATA[ 





<p><strong>En bref :</strong> Que signifie être intelligent ? Cette question prend une tournure nouvelle à l’ère des IA génératives. Dans un article théorique, Craig S. Webster (2025) propose un cadre critique pour penser les différences fondamentales entre intelligence humaine et intelligence artificielle. En retraçant l’histoire des approches en IA, en analysant les limites actuelles des modèles comme ChatGPT, et en introduisant la notion d’« agenda psychotechnique », il offre une boussole pour mieux orienter notre rapport aux technologies dites intelligentes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-06-webster-2025/banner.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>L’intelligence est-elle un calcul ou une émergence ? Est-elle le fruit d’une logique explicite ou d’une adaptation incarnée ? Ce débat, longtemps réservé aux philosophes et aux scientifiques cognitifs, revient sur le devant de la scène avec la prolifération des modèles de langage de grande taille (LLM). Dans un article de réflexion publié dans le tout premier numéro du Journal of Psychology and AI, Craig S. Webster nous invite à dépasser les visions simplistes de l’intelligence, en soulignant les tensions fondamentales entre l’intelligence naturelle — humaine, évolutive, contextuelle — et l’intelligence artificielle — désincarnée, calculatoire, statistique. Comprendre ces tensions, selon lui, est essentiel non seulement pour évaluer les capacités réelles des systèmes IA, mais aussi pour réfléchir à leurs usages concrets en psychologie, en éducation, en justice, ou dans les pratiques sociales plus larges.</p>
</section>
<section id="deux-formes-dintelligence-naturelle-et-artificielle" class="level2">
<h2 class="anchored" data-anchor-id="deux-formes-dintelligence-naturelle-et-artificielle">Deux formes d’intelligence : naturelle et artificielle</h2>
<p>L’intelligence humaine est fondamentalement incarnée. Elle se construit à travers l’évolution biologique, l’expérience sensorielle, les émotions et les interactions sociales. Elle mobilise des systèmes perceptifs, moteurs, affectifs et culturels. Par contraste, l’intelligence artificielle telle qu’implémentée dans les systèmes contemporains repose sur des réseaux neuronaux entraînés par des quantités massives de données. Elle est fondée sur des régularités statistiques, sans perception, sans corps, sans conscience.</p>
<p>Webster illustre cette distinction en s’appuyant sur le modèle bien établi des systèmes cognitifs à double processus, théorisé notamment par les psychologues Daniel Kahneman et Keith Stanovich, utilisé en psychologie pour décrire deux modes complémentaires de fonctionnement de l’esprit humain :</p>
<ul>
<li><p><strong>Système 1</strong> : rapide, intuitif, automatique, contextuel. Il est activé sans effort conscient et repose sur des heuristiques ou règles simples pour interpréter rapidement l’environnement. C’est le système qui nous permet, par exemple, de reconnaître une émotion sur un visage ou de compléter automatiquement une phrase familière. Il est efficace pour prendre des décisions dans des contextes familiers, mais peut aussi être sujet à des biais.</p></li>
<li><p><strong>Système 2</strong> : lent, réfléchi, délibératif. Ce système entre en jeu lorsqu’une tâche demande un raisonnement complexe, une planification, ou un jugement critique. Il permet l’apprentissage abstrait, le raisonnement logique et l’auto-réflexion. Bien qu’il soit plus précis, il est aussi plus coûteux en ressources cognitives et mobilisé plus rarement dans les prises de décision quotidiennes.</p></li>
</ul>
<p>Selon Webster, les modèles actuels d’intelligence artificielle, notamment les modèles de langage comme GPT, reproduisent certains traits du Système 1 : la fluidité, l’apparente cohérence, la capacité à fournir des réponses rapides à partir de grandes bases de données. Toutefois, ils peinent à simuler les fonctions caractéristiques du Système 2. Ils ne peuvent pas raisonner sur plusieurs étapes, planifier des actions en fonction d’objectifs à long terme, ou prendre conscience de leurs propres erreurs. En d’autres termes, ils imitent le comportement humain sans accéder à la compréhension profonde ou à la réflexivité qui caractérisent la pensée humaine authentique.</p>
</section>
<section id="trois-vagues-dia-une-évolution-technique-aux-limites-persistantes" class="level2">
<h2 class="anchored" data-anchor-id="trois-vagues-dia-une-évolution-technique-aux-limites-persistantes">Trois vagues d’IA : une évolution technique aux limites persistantes</h2>
<p>Webster retrace l’histoire des approches en intelligence artificielle à travers trois grandes vagues, chacune fondée sur un paradigme technique et épistémologique distinct :</p>
<ol type="1">
<li><p><strong>L’IA symbolique (1950–1980)</strong> : basée sur la logique formelle, elle repose sur la manipulation de symboles explicites et de règles. Elle vise à reproduire la pensée humaine par déduction logique. C’est l’ère des systèmes logiques, des solveurs, des systèmes d’axiomes.</p></li>
<li><p><strong>Les systèmes experts (années 1980–2000)</strong> : ils utilisent des bases de connaissances alimentées par des experts humains et permettent des inférences abductives (ex. diagnostics médicaux). Leur rigidité et leur dépendance à des règles manuelles limitent leur efficacité dans des contextes dynamiques.</p></li>
<li><p><strong>L’apprentissage machine (depuis les années 2010)</strong> : l’émergence des réseaux neuronaux profonds et de l’apprentissage supervisé à partir de grandes bases de données marque un tournant. Ces systèmes induisent des régularités sans explicitation des règles, et sont aujourd’hui à la base des grands modèles de langage.</p></li>
</ol>
<p>Malgré leurs avancées, aucune de ces approches ne parvient à capturer les aspects incarnés, sociaux et réflexifs de la cognition humaine. L’IA actuelle reste enfermée dans une logique de corrélation sans compréhension.</p>
</section>
<section id="limites-des-modèles-de-langage-illusions-biais-et-opacité" class="level2">
<h2 class="anchored" data-anchor-id="limites-des-modèles-de-langage-illusions-biais-et-opacité">Limites des modèles de langage : illusions, biais et opacité</h2>
<p>Webster s’appuie sur des exemples concrets pour illustrer les faiblesses des LLM. Ainsi, lorsqu’on leur soumet des analogies simples — par exemple, “hamburger = pain + viande, donc salade = ?” — les modèles échouent souvent à produire une réponse cohérente. De même, leur reconnaissance de concepts quotidiens comme « strawberry » peut sembler solide, mais elle repose sur des associations statistiques, non sur une compréhension sémantique réelle et incarnée.</p>
<p>Ces failles révèlent plusieurs limites structurelles importantes :</p>
<ul>
<li><strong>Hallucinations</strong> : les modèles peuvent générer des énoncés faux mais convaincants, créant une illusion de connaissance qui peut tromper même des utilisateurs expérimentés. Ces erreurs ne sont pas des fautes de logique mais le reflet de leur fonctionnement probabiliste.</li>
<li><strong>Biais et opacité</strong> : les données d’entraînement, souvent massives et non contrôlées, peuvent véhiculer des stéréotypes sociaux, culturels ou cognitifs. De plus, le fonctionnement interne des modèles (poids, couches, activations) est difficile à interpréter, ce qui limite la transparence et la responsabilité.</li>
<li><strong>Absence d’ancrage</strong> : contrairement à l’intelligence humaine, qui s’appuie sur des expériences vécues, des perceptions sensorielles et une compréhension corporelle du monde, les LLM ne possèdent ni mémoire personnelle, ni conscience spatiale ou temporelle, ni interaction directe avec le réel. Cela les prive de la possibilité de vérifier, de rectifier ou de contextualiser ce qu’ils produisent.</li>
</ul>
<p>Ces limitations renforcent l’argument selon lequel les LLM ne « comprennent » pas au sens humain. Ils simulent la forme du langage et les apparences du raisonnement, mais sans les fondements cognitifs, émotionnels et perceptifs qui caractérisent la compréhension humaine. En ce sens, ils représentent une forme d’intelligence fonctionnelle, mais profondément différente de celle que nous développons dans notre rapport incarné au monde.</p>
</section>
<section id="lagenda-psychotechnique-repenser-notre-rapport-à-lintelligence-artificielle" class="level2">
<h2 class="anchored" data-anchor-id="lagenda-psychotechnique-repenser-notre-rapport-à-lintelligence-artificielle">L’agenda psychotechnique : repenser notre rapport à l’intelligence artificielle</h2>
<p>Face à ces constats, Webster propose un changement de perspective. Plutôt que de courir après l’utopie d’une intelligence artificielle générale (AGI), il suggère de poser une série de questions fondamentales sur le sens, les fonctions et les usages de l’IA dans nos sociétés contemporaines. Il insiste sur le besoin d’un dialogue entre disciplines pour guider l’évolution de ces technologies de manière éthique, fonctionnelle et socialement bénéfique. C’est ce qu’il appelle un agenda psychotechnique : un programme de recherche interdisciplinaire visant à articuler la psychologie, la philosophie de l’esprit, l’éthique, les sciences cognitives et l’ingénierie.</p>
<p>L’objectif de cet agenda est double : mieux comprendre ce que nous projetons sur l’intelligence artificielle, et mieux concevoir les technologies en fonction des besoins humains réels. Webster appelle à sortir d’une fascination technocentrée pour recentrer la réflexion sur les usages, les effets sociaux et les conditions d’appropriation.</p>
<p>Cet agenda comprend plusieurs axes concrets :</p>
<ul>
<li><strong>Définir les types de cognition utiles</strong> à simuler, sans chercher à reproduire toute l’intelligence humaine. Il s’agit par exemple de distinguer les tâches automatisables (calculs, classifications) de celles qui nécessitent jugement, empathie ou intuition.</li>
<li><strong>Concevoir des systèmes d’IA comme outils collaboratifs</strong>, transparents, auditables et adaptables. L’IA devrait être un partenaire, pas un substitut, et renforcer la capacité d’agir des utilisateurs humains.</li>
<li><strong>Évaluer les usages socialement acceptables</strong>, en tenant compte des contextes culturels, des risques d’automatisation injuste, de surveillance intrusive ou de manipulation des opinions. Cela nécessite des mécanismes de gouvernance et de reddition de comptes.</li>
<li><strong>Favoriser des conceptions incarnées et situées</strong> de l’intelligence artificielle, c’est-à-dire des technologies qui tiennent compte du corps, de l’environnement, et des pratiques sociales dans lesquelles elles s’insèrent. Une IA utile doit être intégrée à un écosystème humain, et non abstraite de celui-ci.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>L’article de Webster n’est pas un manifeste technophobe, mais une invitation à la lucidité. À travers un cadre théorique clair et une mise en perspective historique, il nous aide à mieux comprendre les limites actuelles des IA génératives, tout en suggérant des pistes constructives pour leur évolution. Pour les chercheuses et chercheurs en psychologie, en sciences humaines, ou en technologie, ce texte constitue une base précieuse pour penser le rôle de l’IA dans nos sociétés. Il rappelle aussi que ce qui compte n’est pas seulement ce que l’IA peut faire, mais ce que nous voulons qu’elle fasse — et pour qui.</p>
</section>
<section id="référence-complète" class="level2">
<h2 class="anchored" data-anchor-id="référence-complète">Référence complète</h2>
<p>Webster, C. S. (2025). <em>Natural and artificial intelligence – the psychotechnical agenda of the 21st century</em>. Journal of Psychology and AI, 1(1), 2491445. https://doi.org/10.1080/29974100.2025.2491445</p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>IA générative &amp; cognition</category>
  <category>Éthique, biais et transparence</category>
  <guid>https://benoitplante.github.io/posts/2025-06-06-webster-2025/</guid>
  <pubDate>Fri, 06 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-06-webster-2025/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Acceptation des interventions en santé mentale utilisant l’intelligence artificielle : validation d’un outil de mesure innovant</title>
  <link>https://benoitplante.github.io/posts/2025-06-05-UTAUT-AI/</link>
  <description><![CDATA[ 





<p><strong>En bref :</strong> Une nouvelle étude propose et valide l’UTAUT-AI-DMHI, un outil innovant mesurant l’acceptabilité des interventions numériques en santé mentale utilisant l’intelligence artificielle auprès des patients et des cliniciens. Cet outil, évalué auprès de plus de 1600 participants, identifie sept facteurs clés influençant l’adoption de ces technologies, tels que la facilité d’utilisation, les attentes de qualité thérapeutique et les préoccupations liées à la vie privée. Il pourrait guider le développement de technologies plus adaptées aux besoins et attentes des utilisateurs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-05-UTAUT-AI/banner.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<p>La santé mentale demeure un enjeu majeur, avec une large partie de la population mondiale n’ayant pas accès aux soins nécessaires. Dans ce contexte, une étude récente menée par Vera Békés et ses collègues présente une adaptation innovante du modèle UTAUT (Théorie Unifiée de l’Acceptation et de l’Utilisation des Technologies), spécifiquement destinée à évaluer l’acceptabilité des interventions numériques en santé mentale (DMHI) utilisant l’intelligence artificielle (IA).</p>
<p>Ces interventions, incluant les thérapies par vidéoconférence, les chatbots et les thérapeutes virtuels animés par IA, représentent une solution prometteuse pour combler les lacunes d’accès aux soins. Toutefois, l’efficacité de ces technologies dépend fortement de leur acceptabilité auprès des patients et des cliniciens. Il est ainsi crucial de développer des outils de mesure rigoureux permettant de mieux comprendre les déterminants de cette acceptabilité afin d’améliorer leur conception et de favoriser leur intégration effective dans les pratiques cliniques.</p>
<section id="concepts-centraux-le-modèle-utaut-adapté-à-la-santé-mentale-numérique" class="level2">
<h2 class="anchored" data-anchor-id="concepts-centraux-le-modèle-utaut-adapté-à-la-santé-mentale-numérique">Concepts centraux : le modèle UTAUT adapté à la santé mentale numérique</h2>
<p>L’étude récente menée par Vera Békés et ses collègues s’est appuyée sur la Théorie Unifiée de l’Acceptation et de l’Utilisation des Technologies (UTAUT), développée par Venkatesh et al.&nbsp;(2003). Cette théorie combine plusieurs modèles préexistants pour expliquer les facteurs influençant l’intention des individus à adopter et utiliser une nouvelle technologie. Les principaux déterminants identifiés par l’UTAUT sont l’attente de performance, la facilité d’utilisation, l’influence sociale et les conditions facilitantes. L’étude de Békés propose une adaptation spécifique à la santé mentale numérique et à l’intelligence artificielle : l’UTAUT-AI-DMHI, qui intègre des dimensions additionnelles pertinentes dans ce contexte particulier.</p>
<p>Le modèle UTAUT-AI-DMHI est constitué de sept facteurs déterminants :</p>
<ul>
<li><strong>Facilité d’utilisation</strong> : simplicité et ergonomie.</li>
<li><strong>Influence sociale</strong> : approbation sociale et professionnelle.</li>
<li><strong>Commodité</strong> : praticité, économie de temps et d’argent.</li>
<li><strong>Connexion humaine</strong> : sentiment de lien émotionnel suffisant.</li>
<li><strong>Risque perçu pour la vie privée</strong> : préoccupations sur la confidentialité.</li>
<li><strong>Motivation hédonique</strong> : plaisir ressenti lors de l’utilisation.</li>
<li><strong>Attentes de qualité thérapeutique</strong> : perception de l’efficacité de l’intervention.</li>
</ul>
</section>
<section id="méthodologie-et-validation" class="level2">
<h2 class="anchored" data-anchor-id="méthodologie-et-validation">Méthodologie et validation</h2>
<p>L’étude a impliqué deux échantillons distincts totalisant plus de 1600 participants : un premier composé de 1115 individus incluant 528 patients, 155 cliniciens et 432 personnes appartenant aux deux groupes, ainsi qu’un second échantillon représentatif de 536 participants issus de la population générale américaine. La stratégie d’analyse comprenait des analyses factorielles confirmatoires (CFA) pour tester la structure factorielle hypothétique de l’outil, ainsi que des tests d’invariance de mesure pour vérifier si la mesure était équivalente à travers les différents groupes. La validité prédictive a été évaluée en examinant les corrélations entre les facteurs identifiés et l’intention comportementale d’utiliser les interventions numériques et IA. Les résultats démontrent une excellente fiabilité interne, une validité de construction robuste et une invariance de mesure confirmant que l’UTAUT-AI-DMHI mesure efficacement l’acceptation de divers formats d’interventions numériques et IA auprès de populations diversifiées.</p>
</section>
<section id="résultats-prédire-lintention-dadoption" class="level2">
<h2 class="anchored" data-anchor-id="résultats-prédire-lintention-dadoption">Résultats : prédire l’intention d’adoption</h2>
<p>Les résultats indiquent clairement que chacun des sept facteurs identifiés dans le modèle UTAUT-AI-DMHI présente une association positive significative avec l’intention d’utiliser les interventions en santé mentale numériques et basées sur l’IA. En examinant ces relations de manière détaillée, l’étude révèle que les attentes en matière de qualité thérapeutique constituent le prédicteur le plus puissant de cette intention, illustrant l’importance critique que les utilisateurs accordent à la perception de l’efficacité réelle des interventions. Les facteurs comme la facilité d’utilisation, la commodité, et la connexion humaine apparaissent également fortement liés à l’intention d’utilisation, confirmant que les utilisateurs privilégient des technologies intuitives, pratiques et capables de maintenir une dimension relationnelle satisfaisante. Ces résultats soulignent l’importance d’intégrer systématiquement ces éléments lors de la conception et du déploiement des interventions numériques et IA en santé mentale.</p>
</section>
<section id="implications-cliniques-et-perspectives-pratiques" class="level2">
<h2 class="anchored" data-anchor-id="implications-cliniques-et-perspectives-pratiques">Implications cliniques et perspectives pratiques</h2>
<p>Ces résultats ont des implications concrètes importantes pour les décideurs, les cliniciens et les concepteurs d’interventions numériques en santé mentale. Pour maximiser l’adoption de ces technologies, il est crucial de mettre en place des stratégies visant à renforcer explicitement la confiance des utilisateurs envers leur efficacité clinique, leur facilité d’utilisation, ainsi que leur robustesse en matière de confidentialité et de protection des données personnelles. Il serait également pertinent d’envisager des campagnes d’information et de sensibilisation ciblées pour clarifier les bénéfices et rassurer les utilisateurs quant à la sécurité de ces technologies. De plus, la formation continue des professionnels à travers des programmes spécifiques, ainsi que la mise à disposition de supports techniques adaptés et accessibles, pourraient faciliter leur intégration harmonieuse dans la pratique clinique quotidienne, réduisant ainsi les freins à l’adoption et favorisant une utilisation généralisée et durable.</p>
</section>
<section id="limites-et-perspectives-futures" class="level2">
<h2 class="anchored" data-anchor-id="limites-et-perspectives-futures">Limites et perspectives futures</h2>
<p>L’étude présente quelques limites, notamment la nécessité d’évaluer ces outils dans des contextes culturels variés et à travers des approches mixtes qui prennent en compte les barrières systémiques et organisationnelles. De plus, bien que l’UTAUT-AI-DMHI démontre une bonne validité initiale, des recherches supplémentaires sont nécessaires pour examiner sa sensibilité aux changements dans le temps et son utilité dans des contextes cliniques réels. Il serait également pertinent d’examiner son application auprès de populations vulnérables, incluant les jeunes, les personnes âgées ou les minorités culturelles. Enfin, l’intégration de données qualitatives sur les expériences vécues des utilisateurs permettrait d’enrichir la compréhension des freins et leviers à l’adoption des interventions numériques en santé mentale.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Cet article ouvre des perspectives encourageantes pour l’intégration de l’IA dans la pratique clinique. Il souligne également l’importance de tenir compte des attitudes des utilisateurs dès les premières étapes de développement technologique. L’outil UTAUT-AI-DMHI pourrait devenir une référence clé pour guider l’implémentation réussie de ces interventions dans la santé mentale.</p>
</section>
<section id="référence-complète" class="level2">
<h2 class="anchored" data-anchor-id="référence-complète">Référence complète</h2>
<p>Békés, V., Bőthe, B., &amp; Aafjes-van Doorn, K. (2025). Acceptance of using artificial intelligence and digital technology for mental health interventions: The development and initial validation of the UTAUT‐AI‐DMHI. <em>Clinical Psychology &amp; Psychotherapy</em>, 32, e70085. <a href="https://doi.org/10.1002/cpp.70085" class="uri">https://doi.org/10.1002/cpp.70085</a></p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>Psychométrie</category>
  <category>Psychologie clinique</category>
  <guid>https://benoitplante.github.io/posts/2025-06-05-UTAUT-AI/</guid>
  <pubDate>Thu, 05 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-05-UTAUT-AI/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>L’intelligence émotionnelle des intelligences artificielles : ChatGPT et cie surpassent les humains</title>
  <link>https://benoitplante.github.io/posts/2025-06-03-llm-emotional-test/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-03-llm-emotional-test/banner.png" class="img-fluid figure-img"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="lintelligence-émotionnelle-des-intelligences-artificielles-chatgpt-et-cie-surpassent-les-humains" class="level1">
<h1>L’intelligence émotionnelle des intelligences artificielles : ChatGPT et cie surpassent les humains</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Peut-on vraiment enseigner à une intelligence artificielle à raisonner sur les émotions humaines? Cette question est au cœur d’une étude récente parue dans Communications Psychology (Schlegel et al., 2025), qui explore la capacité des grands modèles de langage (LLM) à réussir et à générer des tests d’intelligence émotionnelle. Les résultats sont saisissants : plusieurs LLM surpassent les humains sur cinq tests psychométriques reconnus, et parviennent à créer des versions alternatives presque équivalentes aux tests originaux.</p>
</section>
<section id="comprendre-lintelligence-émotionnelle" class="level2">
<h2 class="anchored" data-anchor-id="comprendre-lintelligence-émotionnelle">Comprendre l’intelligence émotionnelle</h2>
<p>L’intelligence émotionnelle par habileté (“ability EI”) désigne la capacité à percevoir, comprendre, réguler et exprimer les émotions de manière adaptée dans des situations concrètes. Cette conception repose sur l’idée que l’intelligence émotionnelle est une forme d’intelligence cognitive, mobilisée pour raisonner sur les émotions (les siennes ou celles d’autrui) et pour prendre des décisions sociales appropriées.</p>
<p>Elle inclut, par exemple, la capacité à détecter les signaux émotionnels dans un discours, à identifier la cause d’une émotion complexe, ou à choisir une stratégie de régulation adaptée dans un contexte professionnel. Elle se distingue des modèles de type traits (basés sur des auto-évaluations) par son approche fondée sur la performance et l’évaluation objective. Les personnes ayant une forte EI tendent à mieux s’adapter socialement, à coopérer plus efficacement et à résoudre les conflits de façon constructive. Cette étude s’intéresse à savoir si les LLM peuvent manifester cette forme de compétence, traditionnellement réservée aux humains.</p>
</section>
<section id="la-technologie-mobilisée" class="level2">
<h2 class="anchored" data-anchor-id="la-technologie-mobilisée">La technologie mobilisée</h2>
<p>Les chercheurs ont mis à l’épreuve six LLM : ChatGPT-4, ChatGPT-o1, Gemini 1.5 flash, Copilot 365, Claude 3.5 Haiku et DeepSeek V3. Ces modèles, entraînés sur de vastes corpus de texte, n’ont pas été explicitement programmés pour l’intelligence émotionnelle. Pourtant, ils montrent une capacité surprenante à raisonner sur les émotions, leur régulation, et leurs conséquences sociales.</p>
</section>
<section id="méthodologie-de-létude" class="level2">
<h2 class="anchored" data-anchor-id="méthodologie-de-létude">Méthodologie de l’étude</h2>
<p>L’étude comporte deux volets : (1) évaluer la performance des LLM sur cinq tests d’intelligence émotionnelle, et (2) générer de nouveaux items avec ChatGPT-4 et tester leur validité psychométrique. Les tests utilisés sont :</p>
<ul>
<li><strong>STEM</strong> (gestion des émotions personnelles)</li>
<li><strong>STEU</strong> (compréhension des émotions)</li>
<li><strong>GEMOK-Blends</strong> (reconnaissance d’émotions mixtes)</li>
<li><strong>GECo Regulation</strong> (régulation des émotions en contexte professionnel)</li>
<li><strong>GECo Management</strong> (gestion des émotions chez autrui)</li>
</ul>
<p>Dans le premier volet, chaque LLM a été invité à répondre aux tests selon les instructions originales destinées aux humains. Les chercheurs ont soumis les items dans des sessions séparées et répétées (10 fois par test et par modèle), en s’assurant que chaque modèle donne ses réponses sans biais contextuel. La proportion de bonnes réponses a ensuite été comparée à celle observée dans les échantillons de validation humaine, à l’aide de tests statistiques.</p>
<p>Dans le second volet, ChatGPT-4 a reçu l’instruction de créer des items inédits en se basant sur la structure des tests originaux. Ces items ont été ensuite administrés, de façon aléatoire et en double aveugle, à des participants humains recrutés via la plateforme Prolific (N total = 467). Les performances aux items générés ont été comparées aux performances sur les items originaux selon plusieurs indicateurs : difficulté, clarté, réalisme, diversité du contenu, fidélité interne et validité de construit.</p>
</section>
<section id="résultats-principaux" class="level2">
<h2 class="anchored" data-anchor-id="résultats-principaux">Résultats principaux</h2>
<p>Les LLM ont atteint une précision moyenne de 81 %, contre 56 % pour les humains, soit une différence statistiquement très significative avec des tailles d’effet dépassant d = 1 pour chaque test. Cela signifie que les modèles de langage ont non seulement surpassé les performances humaines moyennes, mais l’ont fait de manière constante sur l’ensemble des dimensions mesurées. Parmi eux, ChatGPT-o1 et DeepSeek V3 se distinguent avec des performances excédant deux écarts-types par rapport aux scores humains, illustrant une maîtrise remarquable des compétences émotionnelles mesurées.</p>
<p>Concernant la création d’items, les tests conçus par ChatGPT-4 ont présenté une difficulté comparable à celle des versions originales, confirmée par des tests d’équivalence (TOST). Ils ont été jugés légèrement plus clairs et plus réalistes par les participants, bien que l’effet soit petit (d &lt; 0,20). En revanche, la diversité du contenu des situations émotionnelles proposées a été légèrement inférieure, les participants ayant regroupé les vignettes générées par ChatGPT en moins de catégories que celles des tests originaux.</p>
<p>La validité de construit (corrélations avec un test de vocabulaire et un autre test d’intelligence émotionnelle) et la fidélité interne (corrélations item-total moyennes) étaient globalement comparables entre les deux versions. Les différences observées restaient inférieures à un effet modéré (d &lt; 0,50), et les versions originales et générées étaient fortement corrélées entre elles (r = 0,46), suggérant qu’elles mesurent les mêmes compétences fondamentales.</p>
</section>
<section id="portée-et-applications-possibles" class="level2">
<h2 class="anchored" data-anchor-id="portée-et-applications-possibles">Portée et applications possibles</h2>
<p>Ces résultats renforcent l’idée que les LLM peuvent jouer un rôle actif dans des interactions socialement sensibles, en particulier dans des contextes où la compréhension des émotions est essentielle pour soutenir la communication, la collaboration ou le bien-être.</p>
<ul>
<li><strong>L’éducation</strong> : des agents pédagogiques intelligents pourraient adapter leurs rétroactions en fonction de l’état émotionnel de l’élève, identifier des signes de démotivation ou d’anxiété, et proposer un soutien à la fois cognitif et affectif.</li>
<li><strong>La santé mentale</strong> : les chatbots empathiques, capables d’identifier les états émotionnels de leurs interlocuteurs et de réagir de manière appropriée, pourraient offrir un soutien psychologique de première ligne, complémentaire aux professionnels.</li>
<li><strong>Le monde du travail</strong> : les LLM pourraient être intégrés à des outils RH pour la gestion des conflits, l’évaluation du climat émotionnel d’une équipe, ou encore pour offrir des conseils personnalisés sur la régulation du stress.</li>
<li><strong>La recherche en psychologie</strong> : les modèles pourraient être utilisés comme simulateurs pour tester des hypothèses théoriques sur les émotions ou pour générer des vignettes émotionnelles variées dans des protocoles expérimentaux.</li>
</ul>
<p>En résumé, les LLM pourraient offrir une forme d’empathie cognitive, stable, systématique, et dénuée des fluctuations affectives qui influencent parfois les jugements humains. Leur capacité à traiter de l’information émotionnelle de manière constante pourrait représenter un atout majeur dans des contextes où la cohérence et la fiabilité sont cruciales.</p>
</section>
<section id="limites-et-perspectives" class="level2">
<h2 class="anchored" data-anchor-id="limites-et-perspectives">Limites et perspectives</h2>
<ul>
<li><strong>Culture</strong> : les tests utilisés ainsi que les données d’entraînement des LLM sont principalement issus de contextes occidentaux, ce qui pose des questions sur la validité interculturelle des compétences mesurées ou simulées.</li>
<li><strong>Complexité des émotions</strong> : les vignettes standardisées ne reflètent que partiellement la variabilité, la subtilité ou l’ambiguïté des situations émotionnelles du quotidien, qui reposent souvent sur des nuances contextuelles.</li>
<li><strong>Transparence</strong> : les processus internes des LLM demeurent largement opaques. On ignore si les modèles raisonnent de manière psychologiquement plausible ou s’ils exploitent simplement des récurrences statistiques.</li>
</ul>
<p>Des recherches futures devront explorer la capacité des LLM à s’adapter à des interactions dynamiques, à intégrer le contexte et l’historique de la relation, et à réagir à des émotions exprimées de manière implicite. Des travaux en psychologie culturelle et en intelligence artificielle explicable seront également essentiels pour évaluer les biais et limites des modèles dans des contextes humains variés.</p>
</section>
<section id="référence" class="level2">
<h2 class="anchored" data-anchor-id="référence">Référence</h2>
<p>Schlegel, K., Sommer, N. R., &amp; Mortillaro, M. (2025). <em>Large language models are proficient in solving and creating emotional intelligence tests</em>. Communications Psychology, 3(80). <a href="https://doi.org/10.1038/s44271-025-00258-x">https://doi.org/10.1038/s44271-025-00258-x</a></p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>
</section>

 ]]></description>
  <category>Méthodologie augmentée par l’IA</category>
  <guid>https://benoitplante.github.io/posts/2025-06-03-llm-emotional-test/</guid>
  <pubDate>Tue, 03 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-03-llm-emotional-test/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Reconnaître la personnalité à partir de textes? Une étude prometteuse avec les modèles transformers</title>
  <link>https://benoitplante.github.io/posts/2025-05-29-personnalite-transformer/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-05-29-personnalite-transformer/banner_article_electra.png" class="img-fluid figure-img"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Peut-on prédire les traits de personnalité d’une personne simplement à partir d’un texte qu’elle a écrit? Cette question, qui pourrait sembler tirée d’un film de science-fiction, est aujourd’hui au cœur de recherches sérieuses en psychologie computationnelle. J’ai récemment lu une étude fascinante présentée à la conférence IEEE ICWR 2025, qui explore cette idée à l’aide d’un modèle d’intelligence artificielle appelé ELECTRA, une version moderne des modèles transformers.</p>
<p>Dans ce billet, je présente l’article en question ainsi que la technologie d’intelligence artificielle qui permet d’analyser les textes pour en extraire des indices de personnalité.</p>
</section>
<section id="un-peu-de-contexte-les-traits-de-personnalité" class="level2">
<h2 class="anchored" data-anchor-id="un-peu-de-contexte-les-traits-de-personnalité">Un peu de contexte : les traits de personnalité</h2>
<p>Dans cet article, les auteurs cherchent à prédire les traits de personnalité en fonction du modèle des Big Five ou « cinq grands facteurs » de la personnalité. Ce modèle propose cinq traits fondamentaux :</p>
<ul>
<li><strong>Ouverture à l’expérience</strong> : imagination, curiosité, créativité</li>
<li><strong>Consciencieux</strong> : organisation, rigueur, discipline</li>
<li><strong>Extraversion</strong> : sociabilité, expressivité, dynamisme</li>
<li><strong>Agréabilité</strong> : bienveillance, coopération, empathie</li>
<li><strong>Névrosisme</strong> : tendance à l’anxiété, la colère ou la tristesse</li>
</ul>
<p>Dans la recherche, on tente souvent de relier ces traits à la manière dont une personne parle ou écrit. Par exemple, quelqu’un de très extraverti pourrait utiliser plus de mots liés à l’émotion ou à la première personne (« je », « moi »), tandis qu’une personne très névrosée pourrait employer davantage de mots à connotation négative.</p>
</section>
<section id="avant-les-transformers-premières-tentatives-danalyse-de-la-personnalité-à-partir-du-texte" class="level2">
<h2 class="anchored" data-anchor-id="avant-les-transformers-premières-tentatives-danalyse-de-la-personnalité-à-partir-du-texte">Avant les transformers : premières tentatives d’analyse de la personnalité à partir du texte</h2>
<section id="des-approches-symboliques-fondées-sur-les-mots" class="level3">
<h3 class="anchored" data-anchor-id="des-approches-symboliques-fondées-sur-les-mots">Des approches symboliques fondées sur les mots</h3>
<p>Avant l’essor de l’apprentissage profond, les chercheur·es ont tenté d’utiliser des approches plus classiques de l’intelligence artificielle pour analyser le lien entre langage et personnalité. Ces méthodes — souvent fondées sur des règles simples ou des statistiques — représentent les premiers pas de l’IA appliquée à la psychologie du langage. Même si elles étaient moins performantes que les approches modernes, elles ont posé les bases de ce champ en plein essor : faire parler les textes pour mieux comprendre qui les écrit.</p>
<p>Les premières tentatives pour relier le langage écrit aux traits de personnalité ont reposé sur des approches dites symboliques ou statistiques. L’une des plus connues est l’utilisation de LIWC (Linguistic Inquiry and Word Count), un outil qui scanne un texte et compte la fréquence de certains types de mots, comme ceux associés à la colère, à la famille, aux émotions positives ou négatives. En analysant ces fréquences, les chercheur·es pouvaient établir des liens avec des traits de personnalité. Par exemple, une personne utilisant souvent des mots chargés d’émotion positive pouvait être perçue comme plus extravertie, alors qu’un vocabulaire plus anxiogène était fréquemment associé au névrosisme.</p>
</section>
<section id="des-modèles-statistiques-plus-flexibles" class="level3">
<h3 class="anchored" data-anchor-id="des-modèles-statistiques-plus-flexibles">Des modèles statistiques plus flexibles</h3>
<p>En parallèle, des méthodes plus statistiques se sont développées. Des modèles comme les forêts aléatoires, les réseaux de neurones artificiels simples ou les SVM (support vector machines) ont été utilisés pour prédire les traits de personnalité à partir de caractéristiques linguistiques extraites manuellement. Ces caractéristiques incluaient, par exemple, le nombre de phrases, la longueur moyenne des mots, la fréquence d’utilisation de certains temps verbaux, ou encore la proportion de pronoms personnels. Ces approches, bien que plus souples que les analyses par mots-clés, restaient fortement dépendantes des choix faits par les chercheurs en amont : il fallait définir quoi mesurer, et comment.</p>
</section>
<section id="les-limites-de-ces-approches" class="level3">
<h3 class="anchored" data-anchor-id="les-limites-de-ces-approches">Les limites de ces approches</h3>
<p>Cependant, ces premières méthodes présentaient plusieurs limites importantes. D’abord, elles s’appuyaient sur de nombreuses hypothèses implicites : il fallait deviner à l’avance quels éléments du langage pourraient être liés à la personnalité. Ensuite, elles étaient peu sensibles au contexte. Un mot comme « froid », par exemple, n’aura pas le même sens selon qu’il décrit la météo ou une relation humaine. Ces modèles peinaient donc à saisir les subtilités du langage naturel. Enfin, leurs performances plafonnaient souvent autour de 65 à 70 % de précision, ce qui limitait leur utilité dans des contextes plus complexes ou variés. C’est précisément pour dépasser ces limites que les chercheurs se sont tournés vers des approches plus flexibles et puissantes, comme celles fondées sur le deep learning. Parmi elles, les modèles transformers marquent une avancée majeure.</p>
</section>
</section>
<section id="comprendre-les-modèles-transformers" class="level2">
<h2 class="anchored" data-anchor-id="comprendre-les-modèles-transformers">Comprendre les modèles transformers</h2>
<p>Avec l’émergence du deep learning, un nouveau type de modèle s’est imposé dans le domaine de la compréhension du langage naturel : le transformer. Introduit en 2017, ce type de modèle a profondément modifié la manière dont les machines analysent les textes, en offrant une capacité inédite à capturer les relations entre les mots, quelle que soit leur position dans la phrase. Contrairement aux anciens modèles qui lisaient les phrases de façon linéaire (mot après mot), les transformers analysent l’ensemble des mots simultanément.</p>
<p>Pour comprendre cela de façon simple, imaginez que vous lisiez une lettre. Le mot « chaud » n’aura pas le même sens dans « une boisson chaude » ou dans « une ambiance chaude ». Le sens dépend des mots qui l’entourent. Les transformers fonctionnent justement sur ce principe : ils accordent à chaque mot une importance différente selon le contexte. Ce mécanisme s’appelle ’attention, et c’est ce qui permet au modèle de repérer quelles parties du texte sont les plus pertinentes pour comprendre un mot donné.</p>
<p>Dans l’étude que je présente ici, les chercheur·es ont utilisé un modèle transformer appelé ELECTRA. Ce modèle est un peu particulier : pour s’entraîner, il joue à un jeu où certains mots du texte sont remplacés par d’autres, et le modèle doit deviner quels mots sont “faux”. Cela l’oblige à comprendre finement la structure et le sens des phrases. Une fois ce modèle pré-entraîné, il peut être spécialisé pour des tâches précises, comme ici, prédire les traits de personnalité à partir de courts textes.</p>
<p>Ce qui rend ELECTRA particulièrement intéressant, c’est qu’il est plus rapide et plus léger que ses prédécesseurs, tout en maintenant une très bonne précision. De plus, il s’agit d’un modèle open-source, ce qui signifie que toute personne intéressée – chercheur·e, praticien·ne ou étudiant·e – peut librement le consulter, l’utiliser ou l’adapter. Il est accessible sur des plateformes comme Hugging Face, favorisant ainsi la démocratisation de l’intelligence artificielle en recherche psychologique. Il est donc adapté à des tâches comme l’analyse psychologique automatisée, où l’on dispose souvent de quantités modérées de données et où l’interprétabilité est essentielle.</p>
</section>
<section id="comment-létude-a-été-réalisée" class="level2">
<h2 class="anchored" data-anchor-id="comment-létude-a-été-réalisée">Comment l’étude a été réalisée</h2>
<p>Les chercheur·es ont utilisé une base de données appelée Pennebaker and King Essays. On y trouve 2 467 petits textes écrits librement par des adultes, souvent sous forme de récits introspectifs ou de réflexions personnelles. Chacun de ces textes est accompagné d’un profil de personnalité établi par questionnaire, basé sur le modèle des Big Five.</p>
<p>Voici comment ils ont procédé :</p>
<ol type="1">
<li><p><strong>Prétraitement</strong> : Les textes bruts ont d’abord été nettoyés pour les rendre exploitables par le modèle. Cela inclut la suppression de caractères non alphabétiques, la normalisation des espaces et ponctuations, et la mise en forme uniforme des textes. Un seuil de longueur a également été fixé pour limiter la variabilité et standardiser l’entrée dans le modèle.</p></li>
<li><p><strong>Augmentation</strong> : Pour compenser la taille relativement modeste du corpus, les chercheur·es ont eu recours à une technique d’augmentation de données. Celle-ci consiste à remplacer certains mots par des synonymes pertinents, extraits de WordNet, de manière aléatoire mais contrôlée (jusqu’à deux remplacements par phrase). Cela permet d’augmenter la diversité lexicale tout en conservant le sens général du texte, ce qui améliore la robustesse du modèle lors de la généralisation à de nouveaux échantillons.</p></li>
<li><p><strong>Entraînement</strong> : Cinq modèles ELECTRA distincts ont été entraînés, chacun dédié à la prédiction d’un des cinq traits de personnalité. En procédant ainsi, les auteur·es ont évité les interférences possibles entre traits (par exemple entre agréabilité et consciencieux) et ont permis au modèle d’apprendre des représentations linguistiques spécifiques à chaque dimension. L’entraînement a été réalisé en utilisant un algorithme d’optimisation de type AdamW, avec une stratégie de régularisation pour éviter le surapprentissage.</p></li>
<li><p><strong>Évaluation</strong> : Les performances des modèles ont été mesurées à l’aide d’un découpage standard des données en ensembles d’entraînement, de validation et de test. Plusieurs métriques ont été utilisées : précision, rappel, score F1 et AUC (aire sous la courbe ROC). Ces indicateurs ont permis de s’assurer que les modèles ne se contentaient pas de bien performer sur les données apprises, mais étaient capables de généraliser sur de nouveaux textes.</p></li>
</ol>
</section>
<section id="les-résultats" class="level2">
<h2 class="anchored" data-anchor-id="les-résultats">Les résultats</h2>
<p>Les modèles ont obtenu de très bons résultats, surpassant les performances d’anciennes approches comme l’analyse lexicale manuelle. Chaque dimension de la personnalité a été considérée indépendamment comme une tâche de classification binaire (par exemple, distinguer un haut d’un bas niveau d’extraversion), avec une évaluation basée sur plusieurs métriques : précision, rappel, score F1 et AUC (aire sous la courbe ROC).</p>
<p>Pour <strong>l’extraversion</strong>, le modèle a obtenu une précision de 78 %, avec une AUC remarquable de 0.84. Cela indique qu’il distingue bien les personnes extraverties des introverties, en captant des indices comme l’usage fréquent de la première personne, l’expressivité et les émotions positives.</p>
<p>Pour <strong>l’ouverture à l’expérience</strong>, la performance atteint 75 %. Le modèle semble particulièrement sensible à la richesse lexicale, à la présence de mots abstraits ou à l’usage de tournures stylistiques variées, souvent associées à ce trait.</p>
<p>Concernant <strong>l’agréabilité</strong>, le modèle atteint également 74 % de précision, avec une AUC solide. Il semble repérer des expressions de politesse, des tournures conciliantes et un ton globalement prosocial.</p>
<p>Pour <strong>le névrosisme</strong>, le score de 74 % reflète une bonne capacité à détecter les marqueurs de tension émotionnelle, comme les mots associés à l’inquiétude, au doute ou à la frustration.</p>
<p>Enfin, <strong>la conscienciosité</strong> est le trait où le modèle obtient la précision la plus faible (72 %), mais tout de même supérieure aux standards classiques. Ce trait semble plus difficile à inférer à partir de textes courts, car il repose sur des indices moins saillants, comme la structure syntaxique, la régularité ou l’organisation du discours.</p>
<p>Ces résultats sont d’autant plus notables que les textes analysés sont brefs, spontanés, et écrits dans un cadre non contraint. La capacité du modèle à extraire de tels signaux à partir d’un matériau aussi variable illustre la puissance de cette approche.</p>
</section>
<section id="pourquoi-cest-prometteur-pour-la-psychologie" class="level2">
<h2 class="anchored" data-anchor-id="pourquoi-cest-prometteur-pour-la-psychologie">Pourquoi c’est prometteur pour la psychologie</h2>
<section id="un-outil-précieux-pour-la-recherche" class="level3">
<h3 class="anchored" data-anchor-id="un-outil-précieux-pour-la-recherche">1. Un outil précieux pour la recherche</h3>
<p>Les chercheur·es en psychologie manipulent souvent de grandes quantités de textes — que ce soit des réponses ouvertes à des questionnaires, des journaux personnels, ou des messages sur des forums en ligne. Analyser tout cela manuellement peut prendre des semaines. Grâce à des modèles comme ELECTRA, on peut obtenir une première lecture automatisée de ces textes : le modèle identifie les tournures, les mots ou les styles d’écriture qui pourraient être liés à certains traits de personnalité. Cela permet de gagner du temps, d’élargir le champ des études, et de formuler de nouvelles hypothèses à partir de données linguistiques qu’on aurait autrement négligées.</p>
</section>
<section id="un-soutien-pour-les-pratiques-cliniques-et-communautaires" class="level3">
<h3 class="anchored" data-anchor-id="un-soutien-pour-les-pratiques-cliniques-et-communautaires">2. Un soutien pour les pratiques cliniques et communautaires</h3>
<p>Dans des contextes comme la clinique, l’intervention communautaire ou même l’éducation, il est souvent utile de comprendre rapidement le vécu ou le profil d’une personne, surtout quand le contact est bref ou se fait en ligne. Un outil comme ELECTRA pourrait, par exemple, analyser de manière discrète les premières réponses écrites d’une personne sur une plateforme de soutien ou dans un formulaire d’accueil. Il pourrait ainsi suggérer certains indicateurs linguistiques liés à l’anxiété, à l’agréabilité ou à l’ouverture d’esprit, ce qui permettrait aux intervenant·es d’adapter leur approche. Cela ne remplace pas le jugement professionnel, mais cela peut offrir un regard complémentaire, plus subtil, pour guider la relation d’aide.</p>
</section>
<section id="vers-des-technologies-plus-sensibles-aux-personnes" class="level3">
<h3 class="anchored" data-anchor-id="vers-des-technologies-plus-sensibles-aux-personnes">3. Vers des technologies plus sensibles aux personnes</h3>
<p>Cette technologie pourrait aussi transformer les interfaces numériques elles-mêmes. Imaginez un agent conversationnel (chatbot) qui ajuste son style de réponse selon la personnalité perçue de la personne avec qui il échange : plus chaleureux, plus structuré, plus créatif… Un tel ajustement pourrait rendre les interactions plus naturelles et plus humaines. C’est une façon de concevoir l’IA non pas comme un outil froid et distant, mais comme un allié dans la création de liens, capable de mieux comprendre et de mieux s’adapter aux personnes.</p>
</section>
</section>
<section id="et-les-limites" class="level2">
<h2 class="anchored" data-anchor-id="et-les-limites">Et les limites?</h2>
<p>Évidemment, tout n’est pas parfait :</p>
<ul>
<li>L’algorithme fonctionne surtout sur des textes en anglais, écrits par une population assez homogène.</li>
<li>Il ne prend en compte que le texte, pas le ton de la voix, le contexte, ni les émotions ressenties.</li>
<li>Le modèle peut surapprendre à ses exemples, ce qui limite parfois sa généralisation.</li>
</ul>
<p>Mais ces défis sont connus, et les auteur·es proposent plusieurs pistes pour y remédier (données plus variées, données multimodales, etc.).</p>
<p>L’article complet est disponible ici : <strong>Saberi, H., Ghofrani, S., &amp; Ravanmehr, R. (2025)</strong>. <em>Personality Recognition Using Transformer Model: A Study on the Big Five Traits</em>. IEEE ICWR. 🔗 <a href="https://ieeexplore.ieee.org/document/11006181">Accès via IEEE Xplore</a></p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>Méthodologie augmentée par l’IA</category>
  <guid>https://benoitplante.github.io/posts/2025-05-29-personnalite-transformer/</guid>
  <pubDate>Mon, 26 May 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-05-29-personnalite-transformer/banner_article_electra.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Quand une IA devient co-chercheuse : réflexions autour de Robin</title>
  <dc:creator>Benoit Plante</dc:creator>
  <link>https://benoitplante.github.io/posts/2025-05-21-ai-robin/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-05-21-ai-robin/banner.png" class="img-fluid figure-img"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="une-découverte-marquante" class="level2">
<h2 class="anchored" data-anchor-id="une-découverte-marquante">Une découverte marquante</h2>
<p>Je suis tombé récemment sur une annonce qui m’a à la fois fasciné et interpellé : le laboratoire <a href="https://www.futurehouse.org">Future House</a>, un groupe de recherche basé à San Francisco qui explore les frontières de la découverte scientifique automatisée, a présenté <em>Robin</em>, une plateforme d’IA capable de réaliser plusieurs étapes clés du travail scientifique. Elle peut lire des articles, formuler des hypothèses, proposer des expériences, analyser les résultats et même suggérer des pistes de recherche à explorer ensuite. Et tout cela se fait à l’aide d’un système dit « multi-agent », avec l’intervention de chercheuses et chercheurs seulement pour les étapes en laboratoire.</p>
<p>Un système multi-agent, c’est un ensemble de petits programmes spécialisés, appelés agents, qui collaborent pour accomplir des tâches complexes. Chacun a un rôle bien précis : certains cherchent dans la littérature scientifique, d’autres analysent les données d’expériences, d’autres encore comparent différentes pistes pour sélectionner les plus prometteuses. Dans le cas de Robin, ces agents s’appellent Crow, Falcon et Finch, et ils sont conçus pour se compléter comme une équipe de recherche virtuelle.</p>
<p>Le preprint associé au projet, intitulé <em>Robin: A Multi-Agent System for Automating Scientific Discovery</em> (Ghareeb et al., 2025), présente une démonstration concrète dans le domaine biomédical : Robin a permis d’identifier un nouveau candidat thérapeutique pour traiter la dégénérescence maculaire liée à l’âge (forme sèche), en proposant une piste qui n’avait jamais été explorée auparavant — le ripasudil, un médicament déjà utilisé pour d’autres troubles oculaires. Tout le raisonnement scientifique, les figures et les analyses présentés dans l’article ont été générés par Robin.</p>
</section>
<section id="quelles-implications-pour-la-psychologie" class="level2">
<h2 class="anchored" data-anchor-id="quelles-implications-pour-la-psychologie">Quelles implications pour la psychologie?</h2>
<p>Cette avancée m’amène à me poser des questions sur ce que cela signifie pour des disciplines comme la psychologie, où les données sont souvent plus complexes, nuancées, et ancrées dans des contextes humains.</p>
<p>Aujourd’hui déjà, je considère que certains outils d’IA agissent comme de véritables partenaires dans nos démarches scientifiques. Je les utilise pour synthétiser de la littérature, organiser des corpus de données, ou encore proposer des structures d’analyse. Nous sommes donc déjà entrés dans une ère de recherche assistée. La nouveauté avec Robin, c’est que l’on entrevoit maintenant la possibilité d’un cycle de recherche entièrement automatisé — du moins dans certains contextes bien circonscrits.</p>
<p>Par exemple, on peut imaginer des systèmes capables d’explorer des bases de données ouvertes, de formuler des hypothèses à partir de connaissances publiées, puis d’analyser des données accessibles publiquement pour confirmer ou rejeter ces hypothèses. Dans le domaine de la psychologie, cela pourrait se faire avec des jeux de données longitudinaux disponibles en libre accès, en lien avec le développement de l’enfant, la santé mentale ou les déterminants sociaux.</p>
<p>Un système comme Robin pourrait théoriquement formuler des hypothèses à partir de ces corpus, effectuer des analyses exploratoires, et suggérer de nouvelles directions de recherche, le tout sans intervention humaine directe — tout en permettant une validation et une interprétation ensuite par des équipes humaines. Cela ne remplacerait pas la recherche participative ou contextuelle, mais cela permettrait de générer de nouvelles connaissances en parallèle, de façon continue.</p>
</section>
<section id="une-transformation-à-suivre-de-près" class="level2">
<h2 class="anchored" data-anchor-id="une-transformation-à-suivre-de-près">Une transformation à suivre de près</h2>
<p>Je vais suivre de près les développements autour de Robin et des autres projets similaires. Ils ouvrent une réflexion profonde sur notre manière de faire de la recherche, sur notre rapport à la connaissance, et sur la façon dont l’IA pourrait enrichir, sans remplacer, notre compréhension du monde. Ces technologies ne nous invitent pas seulement à revoir nos outils, mais à repenser nos méthodologies, nos collaborations et nos modèles de diffusion du savoir. Il se pourrait qu’une partie de la science de demain ne soit pas écrite par des humains, mais validée, partagée et enrichie par eux. C’est une perspective à la fois stimulante et exigeante, qui demande une vigilance éthique et une ouverture intellectuelle constante.</p>
<p><strong>Référence :</strong><br>
Ghareeb, A. E., Chang, B., Mitchener, L., Yiu, A., Szostkiewicz, C. J., Laurent, J. M., … &amp; Rodriques, S. G. (2025). <em>Robin: A multi-agent system for automating scientific discovery</em>. arXiv preprint arXiv:2505.13400. <a href="https://arxiv.org/abs/2505.13400">https://arxiv.org/abs/2505.13400</a></p>
<hr>
<!--Include social share buttons-->
<!-- 
AddToAny check more: https://www.addtoany.com/buttons/for/website 
Using includes will make edits easier, will only need to add or remove button here if needed.
https://quarto.org/docs/authoring/includes.html
-->
<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
<p><a class="a2a_dd" href="https://www.addtoany.com/share"></a> <a class="a2a_button_linkedin"></a> <a class="a2a_button_bluesky"></a> <a class="a2a_button_facebook"></a> <a class="a2a_button_copy_link"></a> <a class="a2a_button_email"></a></p>
</div>
<script async="" src="https://static.addtoany.com/menu/page.js"></script>


</section>

 ]]></description>
  <category>Méthodologie augmentée par l’IA</category>
  <guid>https://benoitplante.github.io/posts/2025-05-21-ai-robin/</guid>
  <pubDate>Wed, 21 May 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-05-21-ai-robin/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
</channel>
</rss>
