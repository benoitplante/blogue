<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Benoit Plante</title>
<link>https://benoitplante.github.io/posts.html</link>
<atom:link href="https://benoitplante.github.io/posts.xml" rel="self" type="application/rss+xml"/>
<description>Réflexions, cas d’usage et outils à l’interface entre IA et psychologie</description>
<generator>quarto-1.7.31</generator>
<lastBuildDate>Fri, 06 Jun 2025 04:00:00 GMT</lastBuildDate>
<item>
  <title>Naturelle ou artificielle ? L’intelligence à l’ère des modèles de langage</title>
  <link>https://benoitplante.github.io/posts/2025-06-06-webster-2025/</link>
  <description><![CDATA[ 





<p><strong>En bref :</strong> Que signifie être intelligent ? Cette question prend une tournure nouvelle à l’ère des IA génératives. Dans un article théorique, Craig S. Webster (2025) propose un cadre critique pour penser les différences fondamentales entre intelligence humaine et intelligence artificielle. En retraçant l’histoire des approches en IA, en analysant les limites actuelles des modèles comme ChatGPT, et en introduisant la notion d’« agenda psychotechnique », il offre une boussole pour mieux orienter notre rapport aux technologies dites intelligentes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-06-webster-2025/banner.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>L’intelligence est-elle un calcul ou une émergence ? Est-elle le fruit d’une logique explicite ou d’une adaptation incarnée ? Ce débat, longtemps réservé aux philosophes et aux scientifiques cognitifs, revient sur le devant de la scène avec la prolifération des modèles de langage de grande taille (LLM). Dans un article de réflexion publié dans le tout premier numéro du Journal of Psychology and AI, Craig S. Webster nous invite à dépasser les visions simplistes de l’intelligence, en soulignant les tensions fondamentales entre l’intelligence naturelle — humaine, évolutive, contextuelle — et l’intelligence artificielle — désincarnée, calculatoire, statistique. Comprendre ces tensions, selon lui, est essentiel non seulement pour évaluer les capacités réelles des systèmes IA, mais aussi pour réfléchir à leurs usages concrets en psychologie, en éducation, en justice, ou dans les pratiques sociales plus larges.</p>
</section>
<section id="deux-formes-dintelligence-naturelle-et-artificielle" class="level2">
<h2 class="anchored" data-anchor-id="deux-formes-dintelligence-naturelle-et-artificielle">Deux formes d’intelligence : naturelle et artificielle</h2>
<p>L’intelligence humaine est fondamentalement incarnée. Elle se construit à travers l’évolution biologique, l’expérience sensorielle, les émotions et les interactions sociales. Elle mobilise des systèmes perceptifs, moteurs, affectifs et culturels. Par contraste, l’intelligence artificielle telle qu’implémentée dans les systèmes contemporains repose sur des réseaux neuronaux entraînés par des quantités massives de données. Elle est fondée sur des régularités statistiques, sans perception, sans corps, sans conscience.</p>
<p>Webster illustre cette distinction en s’appuyant sur le modèle bien établi des systèmes cognitifs à double processus, théorisé notamment par les psychologues Daniel Kahneman et Keith Stanovich, utilisé en psychologie pour décrire deux modes complémentaires de fonctionnement de l’esprit humain :</p>
<ul>
<li><p><strong>Système 1</strong> : rapide, intuitif, automatique, contextuel. Il est activé sans effort conscient et repose sur des heuristiques ou règles simples pour interpréter rapidement l’environnement. C’est le système qui nous permet, par exemple, de reconnaître une émotion sur un visage ou de compléter automatiquement une phrase familière. Il est efficace pour prendre des décisions dans des contextes familiers, mais peut aussi être sujet à des biais.</p></li>
<li><p><strong>Système 2</strong> : lent, réfléchi, délibératif. Ce système entre en jeu lorsqu’une tâche demande un raisonnement complexe, une planification, ou un jugement critique. Il permet l’apprentissage abstrait, le raisonnement logique et l’auto-réflexion. Bien qu’il soit plus précis, il est aussi plus coûteux en ressources cognitives et mobilisé plus rarement dans les prises de décision quotidiennes.</p></li>
</ul>
<p>Selon Webster, les modèles actuels d’intelligence artificielle, notamment les modèles de langage comme GPT, reproduisent certains traits du Système 1 : la fluidité, l’apparente cohérence, la capacité à fournir des réponses rapides à partir de grandes bases de données. Toutefois, ils peinent à simuler les fonctions caractéristiques du Système 2. Ils ne peuvent pas raisonner sur plusieurs étapes, planifier des actions en fonction d’objectifs à long terme, ou prendre conscience de leurs propres erreurs. En d’autres termes, ils imitent le comportement humain sans accéder à la compréhension profonde ou à la réflexivité qui caractérisent la pensée humaine authentique.</p>
</section>
<section id="trois-vagues-dia-une-évolution-technique-aux-limites-persistantes" class="level2">
<h2 class="anchored" data-anchor-id="trois-vagues-dia-une-évolution-technique-aux-limites-persistantes">Trois vagues d’IA : une évolution technique aux limites persistantes</h2>
<p>Webster retrace l’histoire des approches en intelligence artificielle à travers trois grandes vagues, chacune fondée sur un paradigme technique et épistémologique distinct :</p>
<ol type="1">
<li><p><strong>L’IA symbolique (1950–1980)</strong> : basée sur la logique formelle, elle repose sur la manipulation de symboles explicites et de règles. Elle vise à reproduire la pensée humaine par déduction logique. C’est l’ère des systèmes logiques, des solveurs, des systèmes d’axiomes.</p></li>
<li><p><strong>Les systèmes experts (années 1980–2000)</strong> : ils utilisent des bases de connaissances alimentées par des experts humains et permettent des inférences abductives (ex. diagnostics médicaux). Leur rigidité et leur dépendance à des règles manuelles limitent leur efficacité dans des contextes dynamiques.</p></li>
<li><p><strong>L’apprentissage machine (depuis les années 2010)</strong> : l’émergence des réseaux neuronaux profonds et de l’apprentissage supervisé à partir de grandes bases de données marque un tournant. Ces systèmes induisent des régularités sans explicitation des règles, et sont aujourd’hui à la base des grands modèles de langage.</p></li>
</ol>
<p>Malgré leurs avancées, aucune de ces approches ne parvient à capturer les aspects incarnés, sociaux et réflexifs de la cognition humaine. L’IA actuelle reste enfermée dans une logique de corrélation sans compréhension.</p>
</section>
<section id="limites-des-modèles-de-langage-illusions-biais-et-opacité" class="level2">
<h2 class="anchored" data-anchor-id="limites-des-modèles-de-langage-illusions-biais-et-opacité">Limites des modèles de langage : illusions, biais et opacité</h2>
<p>Webster s’appuie sur des exemples concrets pour illustrer les faiblesses des LLM. Ainsi, lorsqu’on leur soumet des analogies simples — par exemple, “hamburger = pain + viande, donc salade = ?” — les modèles échouent souvent à produire une réponse cohérente. De même, leur reconnaissance de concepts quotidiens comme « strawberry » peut sembler solide, mais elle repose sur des associations statistiques, non sur une compréhension sémantique réelle et incarnée.</p>
<p>Ces failles révèlent plusieurs limites structurelles importantes :</p>
<ul>
<li><strong>Hallucinations</strong> : les modèles peuvent générer des énoncés faux mais convaincants, créant une illusion de connaissance qui peut tromper même des utilisateurs expérimentés. Ces erreurs ne sont pas des fautes de logique mais le reflet de leur fonctionnement probabiliste.</li>
<li><strong>Biais et opacité</strong> : les données d’entraînement, souvent massives et non contrôlées, peuvent véhiculer des stéréotypes sociaux, culturels ou cognitifs. De plus, le fonctionnement interne des modèles (poids, couches, activations) est difficile à interpréter, ce qui limite la transparence et la responsabilité.</li>
<li><strong>Absence d’ancrage</strong> : contrairement à l’intelligence humaine, qui s’appuie sur des expériences vécues, des perceptions sensorielles et une compréhension corporelle du monde, les LLM ne possèdent ni mémoire personnelle, ni conscience spatiale ou temporelle, ni interaction directe avec le réel. Cela les prive de la possibilité de vérifier, de rectifier ou de contextualiser ce qu’ils produisent.</li>
</ul>
<p>Ces limitations renforcent l’argument selon lequel les LLM ne « comprennent » pas au sens humain. Ils simulent la forme du langage et les apparences du raisonnement, mais sans les fondements cognitifs, émotionnels et perceptifs qui caractérisent la compréhension humaine. En ce sens, ils représentent une forme d’intelligence fonctionnelle, mais profondément différente de celle que nous développons dans notre rapport incarné au monde.</p>
</section>
<section id="lagenda-psychotechnique-repenser-notre-rapport-à-lintelligence-artificielle" class="level2">
<h2 class="anchored" data-anchor-id="lagenda-psychotechnique-repenser-notre-rapport-à-lintelligence-artificielle">L’agenda psychotechnique : repenser notre rapport à l’intelligence artificielle</h2>
<p>Face à ces constats, Webster propose un changement de perspective. Plutôt que de courir après l’utopie d’une intelligence artificielle générale (AGI), il suggère de poser une série de questions fondamentales sur le sens, les fonctions et les usages de l’IA dans nos sociétés contemporaines. Il insiste sur le besoin d’un dialogue entre disciplines pour guider l’évolution de ces technologies de manière éthique, fonctionnelle et socialement bénéfique. C’est ce qu’il appelle un agenda psychotechnique : un programme de recherche interdisciplinaire visant à articuler la psychologie, la philosophie de l’esprit, l’éthique, les sciences cognitives et l’ingénierie.</p>
<p>L’objectif de cet agenda est double : mieux comprendre ce que nous projetons sur l’intelligence artificielle, et mieux concevoir les technologies en fonction des besoins humains réels. Webster appelle à sortir d’une fascination technocentrée pour recentrer la réflexion sur les usages, les effets sociaux et les conditions d’appropriation.</p>
<p>Cet agenda comprend plusieurs axes concrets :</p>
<ul>
<li><strong>Définir les types de cognition utiles</strong> à simuler, sans chercher à reproduire toute l’intelligence humaine. Il s’agit par exemple de distinguer les tâches automatisables (calculs, classifications) de celles qui nécessitent jugement, empathie ou intuition.</li>
<li><strong>Concevoir des systèmes d’IA comme outils collaboratifs</strong>, transparents, auditables et adaptables. L’IA devrait être un partenaire, pas un substitut, et renforcer la capacité d’agir des utilisateurs humains.</li>
<li><strong>Évaluer les usages socialement acceptables</strong>, en tenant compte des contextes culturels, des risques d’automatisation injuste, de surveillance intrusive ou de manipulation des opinions. Cela nécessite des mécanismes de gouvernance et de reddition de comptes.</li>
<li><strong>Favoriser des conceptions incarnées et situées</strong> de l’intelligence artificielle, c’est-à-dire des technologies qui tiennent compte du corps, de l’environnement, et des pratiques sociales dans lesquelles elles s’insèrent. Une IA utile doit être intégrée à un écosystème humain, et non abstraite de celui-ci.</li>
</ul>
</section>
<section id="conclusion-personnelle" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-personnelle">Conclusion personnelle</h2>
<p>L’article de Webster n’est pas un manifeste technophobe, mais une invitation à la lucidité. À travers un cadre théorique clair et une mise en perspective historique, il nous aide à mieux comprendre les limites actuelles des IA génératives, tout en suggérant des pistes constructives pour leur évolution. Pour les chercheuses et chercheurs en psychologie, en sciences humaines, ou en technologie, ce texte constitue une base précieuse pour penser le rôle de l’IA dans nos sociétés. Il rappelle aussi que ce qui compte n’est pas seulement ce que l’IA peut faire, mais ce que nous voulons qu’elle fasse — et pour qui.</p>
</section>
<section id="référence-complète" class="level2">
<h2 class="anchored" data-anchor-id="référence-complète">Référence complète</h2>
<p>Webster, C. S. (2025). <em>Natural and artificial intelligence – the psychotechnical agenda of the 21st century</em>. Journal of Psychology and AI, 1(1), 2491445. https://doi.org/10.1080/29974100.2025.2491445</p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>IA générative &amp; cognition</category>
  <category>Éthique, biais et transparence</category>
  <guid>https://benoitplante.github.io/posts/2025-06-06-webster-2025/</guid>
  <pubDate>Fri, 06 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-06-webster-2025/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Acceptation des interventions en santé mentale utilisant l’intelligence artificielle : validation d’un outil de mesure innovant</title>
  <link>https://benoitplante.github.io/posts/2025-06-05-UTAUT-AI/</link>
  <description><![CDATA[ 





<p><strong>En bref :</strong> Une nouvelle étude propose et valide l’UTAUT-AI-DMHI, un outil innovant mesurant l’acceptabilité des interventions numériques en santé mentale utilisant l’intelligence artificielle auprès des patients et des cliniciens. Cet outil, évalué auprès de plus de 1600 participants, identifie sept facteurs clés influençant l’adoption de ces technologies, tels que la facilité d’utilisation, les attentes de qualité thérapeutique et les préoccupations liées à la vie privée. Il pourrait guider le développement de technologies plus adaptées aux besoins et attentes des utilisateurs.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-05-UTAUT-AI/banner.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<p>La santé mentale demeure un enjeu majeur, avec une large partie de la population mondiale n’ayant pas accès aux soins nécessaires. Dans ce contexte, une étude récente menée par Vera Békés et ses collègues présente une adaptation innovante du modèle UTAUT (Théorie Unifiée de l’Acceptation et de l’Utilisation des Technologies), spécifiquement destinée à évaluer l’acceptabilité des interventions numériques en santé mentale (DMHI) utilisant l’intelligence artificielle (IA).</p>
<p>Ces interventions, incluant les thérapies par vidéoconférence, les chatbots et les thérapeutes virtuels animés par IA, représentent une solution prometteuse pour combler les lacunes d’accès aux soins. Toutefois, l’efficacité de ces technologies dépend fortement de leur acceptabilité auprès des patients et des cliniciens. Il est ainsi crucial de développer des outils de mesure rigoureux permettant de mieux comprendre les déterminants de cette acceptabilité afin d’améliorer leur conception et de favoriser leur intégration effective dans les pratiques cliniques.</p>
<section id="concepts-centraux-le-modèle-utaut-adapté-à-la-santé-mentale-numérique" class="level2">
<h2 class="anchored" data-anchor-id="concepts-centraux-le-modèle-utaut-adapté-à-la-santé-mentale-numérique">Concepts centraux : le modèle UTAUT adapté à la santé mentale numérique</h2>
<p>L’étude récente menée par Vera Békés et ses collègues s’est appuyée sur la Théorie Unifiée de l’Acceptation et de l’Utilisation des Technologies (UTAUT), développée par Venkatesh et al.&nbsp;(2003). Cette théorie combine plusieurs modèles préexistants pour expliquer les facteurs influençant l’intention des individus à adopter et utiliser une nouvelle technologie. Les principaux déterminants identifiés par l’UTAUT sont l’attente de performance, la facilité d’utilisation, l’influence sociale et les conditions facilitantes. L’étude de Békés propose une adaptation spécifique à la santé mentale numérique et à l’intelligence artificielle : l’UTAUT-AI-DMHI, qui intègre des dimensions additionnelles pertinentes dans ce contexte particulier.</p>
<p>Le modèle UTAUT-AI-DMHI est constitué de sept facteurs déterminants :</p>
<ul>
<li><strong>Facilité d’utilisation</strong> : simplicité et ergonomie.</li>
<li><strong>Influence sociale</strong> : approbation sociale et professionnelle.</li>
<li><strong>Commodité</strong> : praticité, économie de temps et d’argent.</li>
<li><strong>Connexion humaine</strong> : sentiment de lien émotionnel suffisant.</li>
<li><strong>Risque perçu pour la vie privée</strong> : préoccupations sur la confidentialité.</li>
<li><strong>Motivation hédonique</strong> : plaisir ressenti lors de l’utilisation.</li>
<li><strong>Attentes de qualité thérapeutique</strong> : perception de l’efficacité de l’intervention.</li>
</ul>
</section>
<section id="méthodologie-et-validation" class="level2">
<h2 class="anchored" data-anchor-id="méthodologie-et-validation">Méthodologie et validation</h2>
<p>L’étude a impliqué deux échantillons distincts totalisant plus de 1600 participants : un premier composé de 1115 individus incluant 528 patients, 155 cliniciens et 432 personnes appartenant aux deux groupes, ainsi qu’un second échantillon représentatif de 536 participants issus de la population générale américaine. La stratégie d’analyse comprenait des analyses factorielles confirmatoires (CFA) pour tester la structure factorielle hypothétique de l’outil, ainsi que des tests d’invariance de mesure pour vérifier si la mesure était équivalente à travers les différents groupes. La validité prédictive a été évaluée en examinant les corrélations entre les facteurs identifiés et l’intention comportementale d’utiliser les interventions numériques et IA. Les résultats démontrent une excellente fiabilité interne, une validité de construction robuste et une invariance de mesure confirmant que l’UTAUT-AI-DMHI mesure efficacement l’acceptation de divers formats d’interventions numériques et IA auprès de populations diversifiées.</p>
</section>
<section id="résultats-prédire-lintention-dadoption" class="level2">
<h2 class="anchored" data-anchor-id="résultats-prédire-lintention-dadoption">Résultats : prédire l’intention d’adoption</h2>
<p>Les résultats indiquent clairement que chacun des sept facteurs identifiés dans le modèle UTAUT-AI-DMHI présente une association positive significative avec l’intention d’utiliser les interventions en santé mentale numériques et basées sur l’IA. En examinant ces relations de manière détaillée, l’étude révèle que les attentes en matière de qualité thérapeutique constituent le prédicteur le plus puissant de cette intention, illustrant l’importance critique que les utilisateurs accordent à la perception de l’efficacité réelle des interventions. Les facteurs comme la facilité d’utilisation, la commodité, et la connexion humaine apparaissent également fortement liés à l’intention d’utilisation, confirmant que les utilisateurs privilégient des technologies intuitives, pratiques et capables de maintenir une dimension relationnelle satisfaisante. Ces résultats soulignent l’importance d’intégrer systématiquement ces éléments lors de la conception et du déploiement des interventions numériques et IA en santé mentale.</p>
</section>
<section id="implications-cliniques-et-perspectives-pratiques" class="level2">
<h2 class="anchored" data-anchor-id="implications-cliniques-et-perspectives-pratiques">Implications cliniques et perspectives pratiques</h2>
<p>Ces résultats ont des implications concrètes importantes pour les décideurs, les cliniciens et les concepteurs d’interventions numériques en santé mentale. Pour maximiser l’adoption de ces technologies, il est crucial de mettre en place des stratégies visant à renforcer explicitement la confiance des utilisateurs envers leur efficacité clinique, leur facilité d’utilisation, ainsi que leur robustesse en matière de confidentialité et de protection des données personnelles. Il serait également pertinent d’envisager des campagnes d’information et de sensibilisation ciblées pour clarifier les bénéfices et rassurer les utilisateurs quant à la sécurité de ces technologies. De plus, la formation continue des professionnels à travers des programmes spécifiques, ainsi que la mise à disposition de supports techniques adaptés et accessibles, pourraient faciliter leur intégration harmonieuse dans la pratique clinique quotidienne, réduisant ainsi les freins à l’adoption et favorisant une utilisation généralisée et durable.</p>
</section>
<section id="limites-et-perspectives-futures" class="level2">
<h2 class="anchored" data-anchor-id="limites-et-perspectives-futures">Limites et perspectives futures</h2>
<p>L’étude présente quelques limites, notamment la nécessité d’évaluer ces outils dans des contextes culturels variés et à travers des approches mixtes qui prennent en compte les barrières systémiques et organisationnelles. De plus, bien que l’UTAUT-AI-DMHI démontre une bonne validité initiale, des recherches supplémentaires sont nécessaires pour examiner sa sensibilité aux changements dans le temps et son utilité dans des contextes cliniques réels. Il serait également pertinent d’examiner son application auprès de populations vulnérables, incluant les jeunes, les personnes âgées ou les minorités culturelles. Enfin, l’intégration de données qualitatives sur les expériences vécues des utilisateurs permettrait d’enrichir la compréhension des freins et leviers à l’adoption des interventions numériques en santé mentale.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Cet article ouvre des perspectives encourageantes pour l’intégration de l’IA dans la pratique clinique. Il souligne également l’importance de tenir compte des attitudes des utilisateurs dès les premières étapes de développement technologique. L’outil UTAUT-AI-DMHI pourrait devenir une référence clé pour guider l’implémentation réussie de ces interventions dans la santé mentale.</p>
</section>
<section id="référence-complète" class="level2">
<h2 class="anchored" data-anchor-id="référence-complète">Référence complète</h2>
<p>Békés, V., Bőthe, B., &amp; Aafjes-van Doorn, K. (2025). Acceptance of using artificial intelligence and digital technology for mental health interventions: The development and initial validation of the UTAUT‐AI‐DMHI. <em>Clinical Psychology &amp; Psychotherapy</em>, 32, e70085. <a href="https://doi.org/10.1002/cpp.70085" class="uri">https://doi.org/10.1002/cpp.70085</a></p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>Psychométrie</category>
  <category>Psychologie clinique</category>
  <guid>https://benoitplante.github.io/posts/2025-06-05-UTAUT-AI/</guid>
  <pubDate>Thu, 05 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-05-UTAUT-AI/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>L’intelligence émotionnelle des intelligences artificielles : ChatGPT et cie surpassent les humains</title>
  <link>https://benoitplante.github.io/posts/2025-06-03-llm-emotional-test/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-06-03-llm-emotional-test/banner.png" class="img-fluid figure-img"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="lintelligence-émotionnelle-des-intelligences-artificielles-chatgpt-et-cie-surpassent-les-humains" class="level1">
<h1>L’intelligence émotionnelle des intelligences artificielles : ChatGPT et cie surpassent les humains</h1>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Peut-on vraiment enseigner à une intelligence artificielle à raisonner sur les émotions humaines? Cette question est au cœur d’une étude récente parue dans Communications Psychology (Schlegel et al., 2025), qui explore la capacité des grands modèles de langage (LLM) à réussir et à générer des tests d’intelligence émotionnelle. Les résultats sont saisissants : plusieurs LLM surpassent les humains sur cinq tests psychométriques reconnus, et parviennent à créer des versions alternatives presque équivalentes aux tests originaux.</p>
</section>
<section id="comprendre-lintelligence-émotionnelle" class="level2">
<h2 class="anchored" data-anchor-id="comprendre-lintelligence-émotionnelle">Comprendre l’intelligence émotionnelle</h2>
<p>L’intelligence émotionnelle par habileté (“ability EI”) désigne la capacité à percevoir, comprendre, réguler et exprimer les émotions de manière adaptée dans des situations concrètes. Cette conception repose sur l’idée que l’intelligence émotionnelle est une forme d’intelligence cognitive, mobilisée pour raisonner sur les émotions (les siennes ou celles d’autrui) et pour prendre des décisions sociales appropriées.</p>
<p>Elle inclut, par exemple, la capacité à détecter les signaux émotionnels dans un discours, à identifier la cause d’une émotion complexe, ou à choisir une stratégie de régulation adaptée dans un contexte professionnel. Elle se distingue des modèles de type traits (basés sur des auto-évaluations) par son approche fondée sur la performance et l’évaluation objective. Les personnes ayant une forte EI tendent à mieux s’adapter socialement, à coopérer plus efficacement et à résoudre les conflits de façon constructive. Cette étude s’intéresse à savoir si les LLM peuvent manifester cette forme de compétence, traditionnellement réservée aux humains.</p>
</section>
<section id="la-technologie-mobilisée" class="level2">
<h2 class="anchored" data-anchor-id="la-technologie-mobilisée">La technologie mobilisée</h2>
<p>Les chercheurs ont mis à l’épreuve six LLM : ChatGPT-4, ChatGPT-o1, Gemini 1.5 flash, Copilot 365, Claude 3.5 Haiku et DeepSeek V3. Ces modèles, entraînés sur de vastes corpus de texte, n’ont pas été explicitement programmés pour l’intelligence émotionnelle. Pourtant, ils montrent une capacité surprenante à raisonner sur les émotions, leur régulation, et leurs conséquences sociales.</p>
</section>
<section id="méthodologie-de-létude" class="level2">
<h2 class="anchored" data-anchor-id="méthodologie-de-létude">Méthodologie de l’étude</h2>
<p>L’étude comporte deux volets : (1) évaluer la performance des LLM sur cinq tests d’intelligence émotionnelle, et (2) générer de nouveaux items avec ChatGPT-4 et tester leur validité psychométrique. Les tests utilisés sont :</p>
<ul>
<li><strong>STEM</strong> (gestion des émotions personnelles)</li>
<li><strong>STEU</strong> (compréhension des émotions)</li>
<li><strong>GEMOK-Blends</strong> (reconnaissance d’émotions mixtes)</li>
<li><strong>GECo Regulation</strong> (régulation des émotions en contexte professionnel)</li>
<li><strong>GECo Management</strong> (gestion des émotions chez autrui)</li>
</ul>
<p>Dans le premier volet, chaque LLM a été invité à répondre aux tests selon les instructions originales destinées aux humains. Les chercheurs ont soumis les items dans des sessions séparées et répétées (10 fois par test et par modèle), en s’assurant que chaque modèle donne ses réponses sans biais contextuel. La proportion de bonnes réponses a ensuite été comparée à celle observée dans les échantillons de validation humaine, à l’aide de tests statistiques.</p>
<p>Dans le second volet, ChatGPT-4 a reçu l’instruction de créer des items inédits en se basant sur la structure des tests originaux. Ces items ont été ensuite administrés, de façon aléatoire et en double aveugle, à des participants humains recrutés via la plateforme Prolific (N total = 467). Les performances aux items générés ont été comparées aux performances sur les items originaux selon plusieurs indicateurs : difficulté, clarté, réalisme, diversité du contenu, fidélité interne et validité de construit.</p>
</section>
<section id="résultats-principaux" class="level2">
<h2 class="anchored" data-anchor-id="résultats-principaux">Résultats principaux</h2>
<p>Les LLM ont atteint une précision moyenne de 81 %, contre 56 % pour les humains, soit une différence statistiquement très significative avec des tailles d’effet dépassant d = 1 pour chaque test. Cela signifie que les modèles de langage ont non seulement surpassé les performances humaines moyennes, mais l’ont fait de manière constante sur l’ensemble des dimensions mesurées. Parmi eux, ChatGPT-o1 et DeepSeek V3 se distinguent avec des performances excédant deux écarts-types par rapport aux scores humains, illustrant une maîtrise remarquable des compétences émotionnelles mesurées.</p>
<p>Concernant la création d’items, les tests conçus par ChatGPT-4 ont présenté une difficulté comparable à celle des versions originales, confirmée par des tests d’équivalence (TOST). Ils ont été jugés légèrement plus clairs et plus réalistes par les participants, bien que l’effet soit petit (d &lt; 0,20). En revanche, la diversité du contenu des situations émotionnelles proposées a été légèrement inférieure, les participants ayant regroupé les vignettes générées par ChatGPT en moins de catégories que celles des tests originaux.</p>
<p>La validité de construit (corrélations avec un test de vocabulaire et un autre test d’intelligence émotionnelle) et la fidélité interne (corrélations item-total moyennes) étaient globalement comparables entre les deux versions. Les différences observées restaient inférieures à un effet modéré (d &lt; 0,50), et les versions originales et générées étaient fortement corrélées entre elles (r = 0,46), suggérant qu’elles mesurent les mêmes compétences fondamentales.</p>
</section>
<section id="portée-et-applications-possibles" class="level2">
<h2 class="anchored" data-anchor-id="portée-et-applications-possibles">Portée et applications possibles</h2>
<p>Ces résultats renforcent l’idée que les LLM peuvent jouer un rôle actif dans des interactions socialement sensibles, en particulier dans des contextes où la compréhension des émotions est essentielle pour soutenir la communication, la collaboration ou le bien-être.</p>
<ul>
<li><strong>L’éducation</strong> : des agents pédagogiques intelligents pourraient adapter leurs rétroactions en fonction de l’état émotionnel de l’élève, identifier des signes de démotivation ou d’anxiété, et proposer un soutien à la fois cognitif et affectif.</li>
<li><strong>La santé mentale</strong> : les chatbots empathiques, capables d’identifier les états émotionnels de leurs interlocuteurs et de réagir de manière appropriée, pourraient offrir un soutien psychologique de première ligne, complémentaire aux professionnels.</li>
<li><strong>Le monde du travail</strong> : les LLM pourraient être intégrés à des outils RH pour la gestion des conflits, l’évaluation du climat émotionnel d’une équipe, ou encore pour offrir des conseils personnalisés sur la régulation du stress.</li>
<li><strong>La recherche en psychologie</strong> : les modèles pourraient être utilisés comme simulateurs pour tester des hypothèses théoriques sur les émotions ou pour générer des vignettes émotionnelles variées dans des protocoles expérimentaux.</li>
</ul>
<p>En résumé, les LLM pourraient offrir une forme d’empathie cognitive, stable, systématique, et dénuée des fluctuations affectives qui influencent parfois les jugements humains. Leur capacité à traiter de l’information émotionnelle de manière constante pourrait représenter un atout majeur dans des contextes où la cohérence et la fiabilité sont cruciales.</p>
</section>
<section id="limites-et-perspectives" class="level2">
<h2 class="anchored" data-anchor-id="limites-et-perspectives">Limites et perspectives</h2>
<ul>
<li><strong>Culture</strong> : les tests utilisés ainsi que les données d’entraînement des LLM sont principalement issus de contextes occidentaux, ce qui pose des questions sur la validité interculturelle des compétences mesurées ou simulées.</li>
<li><strong>Complexité des émotions</strong> : les vignettes standardisées ne reflètent que partiellement la variabilité, la subtilité ou l’ambiguïté des situations émotionnelles du quotidien, qui reposent souvent sur des nuances contextuelles.</li>
<li><strong>Transparence</strong> : les processus internes des LLM demeurent largement opaques. On ignore si les modèles raisonnent de manière psychologiquement plausible ou s’ils exploitent simplement des récurrences statistiques.</li>
</ul>
<p>Des recherches futures devront explorer la capacité des LLM à s’adapter à des interactions dynamiques, à intégrer le contexte et l’historique de la relation, et à réagir à des émotions exprimées de manière implicite. Des travaux en psychologie culturelle et en intelligence artificielle explicable seront également essentiels pour évaluer les biais et limites des modèles dans des contextes humains variés.</p>
</section>
<section id="référence" class="level2">
<h2 class="anchored" data-anchor-id="référence">Référence</h2>
<p>Schlegel, K., Sommer, N. R., &amp; Mortillaro, M. (2025). <em>Large language models are proficient in solving and creating emotional intelligence tests</em>. Communications Psychology, 3(80). <a href="https://doi.org/10.1038/s44271-025-00258-x">https://doi.org/10.1038/s44271-025-00258-x</a></p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>
</section>

 ]]></description>
  <category>Méthodologie augmentée par l’IA</category>
  <guid>https://benoitplante.github.io/posts/2025-06-03-llm-emotional-test/</guid>
  <pubDate>Tue, 03 Jun 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-06-03-llm-emotional-test/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Reconnaître la personnalité à partir de textes? Une étude prometteuse avec les modèles transformers</title>
  <link>https://benoitplante.github.io/posts/2025-05-29-personnalite-transformer/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-05-29-personnalite-transformer/banner_article_electra.png" class="img-fluid figure-img"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Peut-on prédire les traits de personnalité d’une personne simplement à partir d’un texte qu’elle a écrit? Cette question, qui pourrait sembler tirée d’un film de science-fiction, est aujourd’hui au cœur de recherches sérieuses en psychologie computationnelle. J’ai récemment lu une étude fascinante présentée à la conférence IEEE ICWR 2025, qui explore cette idée à l’aide d’un modèle d’intelligence artificielle appelé ELECTRA, une version moderne des modèles transformers.</p>
<p>Dans ce billet, je présente l’article en question ainsi que la technologie d’intelligence artificielle qui permet d’analyser les textes pour en extraire des indices de personnalité.</p>
</section>
<section id="un-peu-de-contexte-les-traits-de-personnalité" class="level2">
<h2 class="anchored" data-anchor-id="un-peu-de-contexte-les-traits-de-personnalité">Un peu de contexte : les traits de personnalité</h2>
<p>Dans cet article, les auteurs cherchent à prédire les traits de personnalité en fonction du modèle des Big Five ou « cinq grands facteurs » de la personnalité. Ce modèle propose cinq traits fondamentaux :</p>
<ul>
<li><strong>Ouverture à l’expérience</strong> : imagination, curiosité, créativité</li>
<li><strong>Consciencieux</strong> : organisation, rigueur, discipline</li>
<li><strong>Extraversion</strong> : sociabilité, expressivité, dynamisme</li>
<li><strong>Agréabilité</strong> : bienveillance, coopération, empathie</li>
<li><strong>Névrosisme</strong> : tendance à l’anxiété, la colère ou la tristesse</li>
</ul>
<p>Dans la recherche, on tente souvent de relier ces traits à la manière dont une personne parle ou écrit. Par exemple, quelqu’un de très extraverti pourrait utiliser plus de mots liés à l’émotion ou à la première personne (« je », « moi »), tandis qu’une personne très névrosée pourrait employer davantage de mots à connotation négative.</p>
</section>
<section id="avant-les-transformers-premières-tentatives-danalyse-de-la-personnalité-à-partir-du-texte" class="level2">
<h2 class="anchored" data-anchor-id="avant-les-transformers-premières-tentatives-danalyse-de-la-personnalité-à-partir-du-texte">Avant les transformers : premières tentatives d’analyse de la personnalité à partir du texte</h2>
<section id="des-approches-symboliques-fondées-sur-les-mots" class="level3">
<h3 class="anchored" data-anchor-id="des-approches-symboliques-fondées-sur-les-mots">Des approches symboliques fondées sur les mots</h3>
<p>Avant l’essor de l’apprentissage profond, les chercheur·es ont tenté d’utiliser des approches plus classiques de l’intelligence artificielle pour analyser le lien entre langage et personnalité. Ces méthodes — souvent fondées sur des règles simples ou des statistiques — représentent les premiers pas de l’IA appliquée à la psychologie du langage. Même si elles étaient moins performantes que les approches modernes, elles ont posé les bases de ce champ en plein essor : faire parler les textes pour mieux comprendre qui les écrit.</p>
<p>Les premières tentatives pour relier le langage écrit aux traits de personnalité ont reposé sur des approches dites symboliques ou statistiques. L’une des plus connues est l’utilisation de LIWC (Linguistic Inquiry and Word Count), un outil qui scanne un texte et compte la fréquence de certains types de mots, comme ceux associés à la colère, à la famille, aux émotions positives ou négatives. En analysant ces fréquences, les chercheur·es pouvaient établir des liens avec des traits de personnalité. Par exemple, une personne utilisant souvent des mots chargés d’émotion positive pouvait être perçue comme plus extravertie, alors qu’un vocabulaire plus anxiogène était fréquemment associé au névrosisme.</p>
</section>
<section id="des-modèles-statistiques-plus-flexibles" class="level3">
<h3 class="anchored" data-anchor-id="des-modèles-statistiques-plus-flexibles">Des modèles statistiques plus flexibles</h3>
<p>En parallèle, des méthodes plus statistiques se sont développées. Des modèles comme les forêts aléatoires, les réseaux de neurones artificiels simples ou les SVM (support vector machines) ont été utilisés pour prédire les traits de personnalité à partir de caractéristiques linguistiques extraites manuellement. Ces caractéristiques incluaient, par exemple, le nombre de phrases, la longueur moyenne des mots, la fréquence d’utilisation de certains temps verbaux, ou encore la proportion de pronoms personnels. Ces approches, bien que plus souples que les analyses par mots-clés, restaient fortement dépendantes des choix faits par les chercheurs en amont : il fallait définir quoi mesurer, et comment.</p>
</section>
<section id="les-limites-de-ces-approches" class="level3">
<h3 class="anchored" data-anchor-id="les-limites-de-ces-approches">Les limites de ces approches</h3>
<p>Cependant, ces premières méthodes présentaient plusieurs limites importantes. D’abord, elles s’appuyaient sur de nombreuses hypothèses implicites : il fallait deviner à l’avance quels éléments du langage pourraient être liés à la personnalité. Ensuite, elles étaient peu sensibles au contexte. Un mot comme « froid », par exemple, n’aura pas le même sens selon qu’il décrit la météo ou une relation humaine. Ces modèles peinaient donc à saisir les subtilités du langage naturel. Enfin, leurs performances plafonnaient souvent autour de 65 à 70 % de précision, ce qui limitait leur utilité dans des contextes plus complexes ou variés. C’est précisément pour dépasser ces limites que les chercheurs se sont tournés vers des approches plus flexibles et puissantes, comme celles fondées sur le deep learning. Parmi elles, les modèles transformers marquent une avancée majeure.</p>
</section>
</section>
<section id="comprendre-les-modèles-transformers" class="level2">
<h2 class="anchored" data-anchor-id="comprendre-les-modèles-transformers">Comprendre les modèles transformers</h2>
<p>Avec l’émergence du deep learning, un nouveau type de modèle s’est imposé dans le domaine de la compréhension du langage naturel : le transformer. Introduit en 2017, ce type de modèle a profondément modifié la manière dont les machines analysent les textes, en offrant une capacité inédite à capturer les relations entre les mots, quelle que soit leur position dans la phrase. Contrairement aux anciens modèles qui lisaient les phrases de façon linéaire (mot après mot), les transformers analysent l’ensemble des mots simultanément.</p>
<p>Pour comprendre cela de façon simple, imaginez que vous lisiez une lettre. Le mot « chaud » n’aura pas le même sens dans « une boisson chaude » ou dans « une ambiance chaude ». Le sens dépend des mots qui l’entourent. Les transformers fonctionnent justement sur ce principe : ils accordent à chaque mot une importance différente selon le contexte. Ce mécanisme s’appelle ’attention, et c’est ce qui permet au modèle de repérer quelles parties du texte sont les plus pertinentes pour comprendre un mot donné.</p>
<p>Dans l’étude que je présente ici, les chercheur·es ont utilisé un modèle transformer appelé ELECTRA. Ce modèle est un peu particulier : pour s’entraîner, il joue à un jeu où certains mots du texte sont remplacés par d’autres, et le modèle doit deviner quels mots sont “faux”. Cela l’oblige à comprendre finement la structure et le sens des phrases. Une fois ce modèle pré-entraîné, il peut être spécialisé pour des tâches précises, comme ici, prédire les traits de personnalité à partir de courts textes.</p>
<p>Ce qui rend ELECTRA particulièrement intéressant, c’est qu’il est plus rapide et plus léger que ses prédécesseurs, tout en maintenant une très bonne précision. De plus, il s’agit d’un modèle open-source, ce qui signifie que toute personne intéressée – chercheur·e, praticien·ne ou étudiant·e – peut librement le consulter, l’utiliser ou l’adapter. Il est accessible sur des plateformes comme Hugging Face, favorisant ainsi la démocratisation de l’intelligence artificielle en recherche psychologique. Il est donc adapté à des tâches comme l’analyse psychologique automatisée, où l’on dispose souvent de quantités modérées de données et où l’interprétabilité est essentielle.</p>
</section>
<section id="comment-létude-a-été-réalisée" class="level2">
<h2 class="anchored" data-anchor-id="comment-létude-a-été-réalisée">Comment l’étude a été réalisée</h2>
<p>Les chercheur·es ont utilisé une base de données appelée Pennebaker and King Essays. On y trouve 2 467 petits textes écrits librement par des adultes, souvent sous forme de récits introspectifs ou de réflexions personnelles. Chacun de ces textes est accompagné d’un profil de personnalité établi par questionnaire, basé sur le modèle des Big Five.</p>
<p>Voici comment ils ont procédé :</p>
<ol type="1">
<li><p><strong>Prétraitement</strong> : Les textes bruts ont d’abord été nettoyés pour les rendre exploitables par le modèle. Cela inclut la suppression de caractères non alphabétiques, la normalisation des espaces et ponctuations, et la mise en forme uniforme des textes. Un seuil de longueur a également été fixé pour limiter la variabilité et standardiser l’entrée dans le modèle.</p></li>
<li><p><strong>Augmentation</strong> : Pour compenser la taille relativement modeste du corpus, les chercheur·es ont eu recours à une technique d’augmentation de données. Celle-ci consiste à remplacer certains mots par des synonymes pertinents, extraits de WordNet, de manière aléatoire mais contrôlée (jusqu’à deux remplacements par phrase). Cela permet d’augmenter la diversité lexicale tout en conservant le sens général du texte, ce qui améliore la robustesse du modèle lors de la généralisation à de nouveaux échantillons.</p></li>
<li><p><strong>Entraînement</strong> : Cinq modèles ELECTRA distincts ont été entraînés, chacun dédié à la prédiction d’un des cinq traits de personnalité. En procédant ainsi, les auteur·es ont évité les interférences possibles entre traits (par exemple entre agréabilité et consciencieux) et ont permis au modèle d’apprendre des représentations linguistiques spécifiques à chaque dimension. L’entraînement a été réalisé en utilisant un algorithme d’optimisation de type AdamW, avec une stratégie de régularisation pour éviter le surapprentissage.</p></li>
<li><p><strong>Évaluation</strong> : Les performances des modèles ont été mesurées à l’aide d’un découpage standard des données en ensembles d’entraînement, de validation et de test. Plusieurs métriques ont été utilisées : précision, rappel, score F1 et AUC (aire sous la courbe ROC). Ces indicateurs ont permis de s’assurer que les modèles ne se contentaient pas de bien performer sur les données apprises, mais étaient capables de généraliser sur de nouveaux textes.</p></li>
</ol>
</section>
<section id="les-résultats" class="level2">
<h2 class="anchored" data-anchor-id="les-résultats">Les résultats</h2>
<p>Les modèles ont obtenu de très bons résultats, surpassant les performances d’anciennes approches comme l’analyse lexicale manuelle. Chaque dimension de la personnalité a été considérée indépendamment comme une tâche de classification binaire (par exemple, distinguer un haut d’un bas niveau d’extraversion), avec une évaluation basée sur plusieurs métriques : précision, rappel, score F1 et AUC (aire sous la courbe ROC).</p>
<p>Pour <strong>l’extraversion</strong>, le modèle a obtenu une précision de 78 %, avec une AUC remarquable de 0.84. Cela indique qu’il distingue bien les personnes extraverties des introverties, en captant des indices comme l’usage fréquent de la première personne, l’expressivité et les émotions positives.</p>
<p>Pour <strong>l’ouverture à l’expérience</strong>, la performance atteint 75 %. Le modèle semble particulièrement sensible à la richesse lexicale, à la présence de mots abstraits ou à l’usage de tournures stylistiques variées, souvent associées à ce trait.</p>
<p>Concernant <strong>l’agréabilité</strong>, le modèle atteint également 74 % de précision, avec une AUC solide. Il semble repérer des expressions de politesse, des tournures conciliantes et un ton globalement prosocial.</p>
<p>Pour <strong>le névrosisme</strong>, le score de 74 % reflète une bonne capacité à détecter les marqueurs de tension émotionnelle, comme les mots associés à l’inquiétude, au doute ou à la frustration.</p>
<p>Enfin, <strong>la conscienciosité</strong> est le trait où le modèle obtient la précision la plus faible (72 %), mais tout de même supérieure aux standards classiques. Ce trait semble plus difficile à inférer à partir de textes courts, car il repose sur des indices moins saillants, comme la structure syntaxique, la régularité ou l’organisation du discours.</p>
<p>Ces résultats sont d’autant plus notables que les textes analysés sont brefs, spontanés, et écrits dans un cadre non contraint. La capacité du modèle à extraire de tels signaux à partir d’un matériau aussi variable illustre la puissance de cette approche.</p>
</section>
<section id="pourquoi-cest-prometteur-pour-la-psychologie" class="level2">
<h2 class="anchored" data-anchor-id="pourquoi-cest-prometteur-pour-la-psychologie">Pourquoi c’est prometteur pour la psychologie</h2>
<section id="un-outil-précieux-pour-la-recherche" class="level3">
<h3 class="anchored" data-anchor-id="un-outil-précieux-pour-la-recherche">1. Un outil précieux pour la recherche</h3>
<p>Les chercheur·es en psychologie manipulent souvent de grandes quantités de textes — que ce soit des réponses ouvertes à des questionnaires, des journaux personnels, ou des messages sur des forums en ligne. Analyser tout cela manuellement peut prendre des semaines. Grâce à des modèles comme ELECTRA, on peut obtenir une première lecture automatisée de ces textes : le modèle identifie les tournures, les mots ou les styles d’écriture qui pourraient être liés à certains traits de personnalité. Cela permet de gagner du temps, d’élargir le champ des études, et de formuler de nouvelles hypothèses à partir de données linguistiques qu’on aurait autrement négligées.</p>
</section>
<section id="un-soutien-pour-les-pratiques-cliniques-et-communautaires" class="level3">
<h3 class="anchored" data-anchor-id="un-soutien-pour-les-pratiques-cliniques-et-communautaires">2. Un soutien pour les pratiques cliniques et communautaires</h3>
<p>Dans des contextes comme la clinique, l’intervention communautaire ou même l’éducation, il est souvent utile de comprendre rapidement le vécu ou le profil d’une personne, surtout quand le contact est bref ou se fait en ligne. Un outil comme ELECTRA pourrait, par exemple, analyser de manière discrète les premières réponses écrites d’une personne sur une plateforme de soutien ou dans un formulaire d’accueil. Il pourrait ainsi suggérer certains indicateurs linguistiques liés à l’anxiété, à l’agréabilité ou à l’ouverture d’esprit, ce qui permettrait aux intervenant·es d’adapter leur approche. Cela ne remplace pas le jugement professionnel, mais cela peut offrir un regard complémentaire, plus subtil, pour guider la relation d’aide.</p>
</section>
<section id="vers-des-technologies-plus-sensibles-aux-personnes" class="level3">
<h3 class="anchored" data-anchor-id="vers-des-technologies-plus-sensibles-aux-personnes">3. Vers des technologies plus sensibles aux personnes</h3>
<p>Cette technologie pourrait aussi transformer les interfaces numériques elles-mêmes. Imaginez un agent conversationnel (chatbot) qui ajuste son style de réponse selon la personnalité perçue de la personne avec qui il échange : plus chaleureux, plus structuré, plus créatif… Un tel ajustement pourrait rendre les interactions plus naturelles et plus humaines. C’est une façon de concevoir l’IA non pas comme un outil froid et distant, mais comme un allié dans la création de liens, capable de mieux comprendre et de mieux s’adapter aux personnes.</p>
</section>
</section>
<section id="et-les-limites" class="level2">
<h2 class="anchored" data-anchor-id="et-les-limites">Et les limites?</h2>
<p>Évidemment, tout n’est pas parfait :</p>
<ul>
<li>L’algorithme fonctionne surtout sur des textes en anglais, écrits par une population assez homogène.</li>
<li>Il ne prend en compte que le texte, pas le ton de la voix, le contexte, ni les émotions ressenties.</li>
<li>Le modèle peut surapprendre à ses exemples, ce qui limite parfois sa généralisation.</li>
</ul>
<p>Mais ces défis sont connus, et les auteur·es proposent plusieurs pistes pour y remédier (données plus variées, données multimodales, etc.).</p>
<p>L’article complet est disponible ici : <strong>Saberi, H., Ghofrani, S., &amp; Ravanmehr, R. (2025)</strong>. <em>Personality Recognition Using Transformer Model: A Study on the Big Five Traits</em>. IEEE ICWR. 🔗 <a href="https://ieeexplore.ieee.org/document/11006181">Accès via IEEE Xplore</a></p>
<!-- Formulaire d’abonnement Brevo -->
<iframe width="540" height="405" scrolling="no" src="https://sibforms.com/serve/MUIFAJuXpDvH14nAsFXhXmM7v_z4nHcpDJCxRYobbS4dO7G-ovnmEkzoaPhHHEKEPAWRf3EVMvbOumRBiEsM6A50GTewyamCczEPOkwY9jSzdOIhDlnGvyrZJq7_DnhQswAXMCQ4QhEhVv0wZoQ_S-DisWk4a4YeHj6TW3XrELrEZPr4Nv-e2EJt60iSgcFerHiFJzCrIkdm7njy" frameborder="0" allowfullscreen="" style="display: block;margin-left: auto;margin-right: auto;max-width: 100%;">
</iframe>


</section>

 ]]></description>
  <category>Méthodologie augmentée par l’IA</category>
  <guid>https://benoitplante.github.io/posts/2025-05-29-personnalite-transformer/</guid>
  <pubDate>Mon, 26 May 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-05-29-personnalite-transformer/banner_article_electra.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Quand une IA devient co-chercheuse : réflexions autour de Robin</title>
  <dc:creator>Benoit Plante</dc:creator>
  <link>https://benoitplante.github.io/posts/2025-05-21-ai-robin/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://benoitplante.github.io/posts/2025-05-21-ai-robin/banner.png" class="img-fluid figure-img"></p>
<figcaption>Générée par ChatGPT</figcaption>
</figure>
</div>
<section id="une-découverte-marquante" class="level2">
<h2 class="anchored" data-anchor-id="une-découverte-marquante">Une découverte marquante</h2>
<p>Je suis tombé récemment sur une annonce qui m’a à la fois fasciné et interpellé : le laboratoire <a href="https://www.futurehouse.org">Future House</a>, un groupe de recherche basé à San Francisco qui explore les frontières de la découverte scientifique automatisée, a présenté <em>Robin</em>, une plateforme d’IA capable de réaliser plusieurs étapes clés du travail scientifique. Elle peut lire des articles, formuler des hypothèses, proposer des expériences, analyser les résultats et même suggérer des pistes de recherche à explorer ensuite. Et tout cela se fait à l’aide d’un système dit « multi-agent », avec l’intervention de chercheuses et chercheurs seulement pour les étapes en laboratoire.</p>
<p>Un système multi-agent, c’est un ensemble de petits programmes spécialisés, appelés agents, qui collaborent pour accomplir des tâches complexes. Chacun a un rôle bien précis : certains cherchent dans la littérature scientifique, d’autres analysent les données d’expériences, d’autres encore comparent différentes pistes pour sélectionner les plus prometteuses. Dans le cas de Robin, ces agents s’appellent Crow, Falcon et Finch, et ils sont conçus pour se compléter comme une équipe de recherche virtuelle.</p>
<p>Le preprint associé au projet, intitulé <em>Robin: A Multi-Agent System for Automating Scientific Discovery</em> (Ghareeb et al., 2025), présente une démonstration concrète dans le domaine biomédical : Robin a permis d’identifier un nouveau candidat thérapeutique pour traiter la dégénérescence maculaire liée à l’âge (forme sèche), en proposant une piste qui n’avait jamais été explorée auparavant — le ripasudil, un médicament déjà utilisé pour d’autres troubles oculaires. Tout le raisonnement scientifique, les figures et les analyses présentés dans l’article ont été générés par Robin.</p>
</section>
<section id="quelles-implications-pour-la-psychologie" class="level2">
<h2 class="anchored" data-anchor-id="quelles-implications-pour-la-psychologie">Quelles implications pour la psychologie?</h2>
<p>Cette avancée m’amène à me poser des questions sur ce que cela signifie pour des disciplines comme la psychologie, où les données sont souvent plus complexes, nuancées, et ancrées dans des contextes humains.</p>
<p>Aujourd’hui déjà, je considère que certains outils d’IA agissent comme de véritables partenaires dans nos démarches scientifiques. Je les utilise pour synthétiser de la littérature, organiser des corpus de données, ou encore proposer des structures d’analyse. Nous sommes donc déjà entrés dans une ère de recherche assistée. La nouveauté avec Robin, c’est que l’on entrevoit maintenant la possibilité d’un cycle de recherche entièrement automatisé — du moins dans certains contextes bien circonscrits.</p>
<p>Par exemple, on peut imaginer des systèmes capables d’explorer des bases de données ouvertes, de formuler des hypothèses à partir de connaissances publiées, puis d’analyser des données accessibles publiquement pour confirmer ou rejeter ces hypothèses. Dans le domaine de la psychologie, cela pourrait se faire avec des jeux de données longitudinaux disponibles en libre accès, en lien avec le développement de l’enfant, la santé mentale ou les déterminants sociaux.</p>
<p>Un système comme Robin pourrait théoriquement formuler des hypothèses à partir de ces corpus, effectuer des analyses exploratoires, et suggérer de nouvelles directions de recherche, le tout sans intervention humaine directe — tout en permettant une validation et une interprétation ensuite par des équipes humaines. Cela ne remplacerait pas la recherche participative ou contextuelle, mais cela permettrait de générer de nouvelles connaissances en parallèle, de façon continue.</p>
</section>
<section id="une-transformation-à-suivre-de-près" class="level2">
<h2 class="anchored" data-anchor-id="une-transformation-à-suivre-de-près">Une transformation à suivre de près</h2>
<p>Je vais suivre de près les développements autour de Robin et des autres projets similaires. Ils ouvrent une réflexion profonde sur notre manière de faire de la recherche, sur notre rapport à la connaissance, et sur la façon dont l’IA pourrait enrichir, sans remplacer, notre compréhension du monde. Ces technologies ne nous invitent pas seulement à revoir nos outils, mais à repenser nos méthodologies, nos collaborations et nos modèles de diffusion du savoir. Il se pourrait qu’une partie de la science de demain ne soit pas écrite par des humains, mais validée, partagée et enrichie par eux. C’est une perspective à la fois stimulante et exigeante, qui demande une vigilance éthique et une ouverture intellectuelle constante.</p>
<p><strong>Référence :</strong><br>
Ghareeb, A. E., Chang, B., Mitchener, L., Yiu, A., Szostkiewicz, C. J., Laurent, J. M., … &amp; Rodriques, S. G. (2025). <em>Robin: A multi-agent system for automating scientific discovery</em>. arXiv preprint arXiv:2505.13400. <a href="https://arxiv.org/abs/2505.13400">https://arxiv.org/abs/2505.13400</a></p>
<hr>
<!--Include social share buttons-->
<!-- 
AddToAny check more: https://www.addtoany.com/buttons/for/website 
Using includes will make edits easier, will only need to add or remove button here if needed.
https://quarto.org/docs/authoring/includes.html
-->
<div class="a2a_kit a2a_kit_size_32 a2a_default_style">
<p><a class="a2a_dd" href="https://www.addtoany.com/share"></a> <a class="a2a_button_linkedin"></a> <a class="a2a_button_bluesky"></a> <a class="a2a_button_facebook"></a> <a class="a2a_button_copy_link"></a> <a class="a2a_button_email"></a></p>
</div>
<script async="" src="https://static.addtoany.com/menu/page.js"></script>


</section>

 ]]></description>
  <category>Méthodologie augmentée par l’IA</category>
  <guid>https://benoitplante.github.io/posts/2025-05-21-ai-robin/</guid>
  <pubDate>Wed, 21 May 2025 04:00:00 GMT</pubDate>
  <media:content url="https://benoitplante.github.io/posts/2025-05-21-ai-robin/banner.png" medium="image" type="image/png" height="96" width="144"/>
</item>
</channel>
</rss>
